# rf_iter_only_geo_stratified.py
# ------------------------------------------------------------
# ç›®æ ‡ï¼šä»…åšéšæœºæ£®æ—ï¼ˆRFï¼‰éƒ¨åˆ†çš„â€œç‰¹å¾é€‰æ‹© + è¿­ä»£è°ƒä¼˜â€
# - è¾“å…¥ï¼šroads_with_density.geojsonï¼ˆå«å…¨ç½‘ç‰¹å¾ï¼‰ï¼Œosm_id_with_aadt.csvï¼ˆID + AADTè§‚æµ‹ï¼‰
# - ç©ºé—´å‡åŒ€ 80/20 æ‹†åˆ†ï¼ˆKMeans ç©ºé—´ç°‡ + åˆ†ç°‡æŠ½æ ·ï¼Œç¡®ä¿ç©ºé—´ç‹¬ç«‹ï¼‰
# - è¿­ä»£ï¼šéšæœºæœç´¢è¶…å‚ + åŸºäºå†…ç½®é‡è¦æ€§çš„â€œç´¯è®¡è¦†ç›–ç‡â€è£å‰ª
# - è¯„ä¼°ï¼šä»…ç”¨éšæœºæ£®æ—çš„æµ‹è¯•é›† RÂ² / RMSE
# - è¾“å‡ºï¼šæ¯è½®çš„ç‰¹å¾é‡è¦æ€§ CSV + å†å²è®°å½• + æœ€ä¼˜æ¨¡å‹é‡è¦æ€§ï¼ˆå«ç½®æ¢é‡è¦æ€§ï¼‰
# ------------------------------------------------------------

import warnings
warnings.filterwarnings("ignore")

import math
import numpy as np
import pandas as pd
import geopandas as gpd

from shapely.geometry import LineString, MultiLineString, Point
from sklearn.model_selection import GroupKFold, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.cluster import KMeans
from sklearn.inspection import permutation_importance
from scipy.stats import randint, uniform

# =================== é…ç½® ===================
ROADS_PATH = "roads_with_density.geojson"
CSV_PATH   = "osm_id_with_aadt.csv"

CRS_METRIC  = 32647     # æ›¼è°· UTM 47Nï¼ˆç±³ï¼‰
TEST_RATIO  = 0.20
RANDOM_SEED = 42

# è¿­ä»£ä¸è°ƒå‚
MAX_ROUNDS          = 5      # æœ€å¤šè¿­ä»£è½®æ•°
KEEP_CUM_IMPORTANCE = 0.95   # ç´¯è®¡é‡è¦æ€§è¦†ç›–é˜ˆå€¼
MIN_IMPORTANCE      = 1e-4   # æå°é‡è¦æ€§å‰”é™¤é˜ˆå€¼
EARLY_STOP_ROUNDS   = 2      # è¿ç»­å‡ è½®æ— æå‡åˆ™åœæ­¢
N_RANDOM_SEARCH     = 30     # æ¯è½®è¶…å‚éšæœºæœç´¢æ¬¡æ•°
N_PERM_REPEATS      = 10     # ç½®æ¢é‡è¦æ€§é‡å¤æ¬¡æ•°

# =================== å·¥å…·å‡½æ•° ===================
def find_id_key(roads_df, csv_df):
    candidates = ["osm_id", "osmid", "id", "road_id", "roadid"]
    roads_cols = {c.lower(): c for c in roads_df.columns}
    csv_cols   = {c.lower(): c for c in csv_df.columns}
    for lc in candidates:
        if lc in roads_cols and lc in csv_cols:
            return roads_cols[lc], csv_cols[lc]
    inter = set(roads_cols.keys()) & set(csv_cols.keys())
    if inter:
        lc = list(inter)[0]
        return roads_cols[lc], csv_cols[lc]
    raise ValueError("æ‰¾ä¸åˆ°å…±åŒIDåˆ—ï¼›è¯·ç¡®ä¿ä¸¤è¡¨éƒ½åŒ…å«ç›¸åŒçš„ ID å­—æ®µï¼ˆå¦‚ osm_idï¼‰ã€‚")

def find_aadt_col(csv_df):
    for c in ["aadt","AADT","aadt_value","AADT_value","value"]:
        if c in csv_df.columns: return c
    return csv_df.columns[-1]

def numeric_maxspeed(series):
    s = series.astype(str).str.extract(r"(\d+\.?\d*)", expand=False)
    return pd.to_numeric(s, errors="coerce")

def choose_poi_columns(df):
    prefixes = ("front20_", "dens100_", "dens300_", "dens800_", "dist_", "entropy_")
    return [c for c in df.columns if c.startswith(prefixes)]

def make_ohe():
    # å…¼å®¹ä¸åŒ sklearn ç‰ˆæœ¬çš„ OneHotEncoder
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse=False)

def make_reg_pipeline(num_cols, cat_cols, random_state=RANDOM_SEED):
    prep = ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", make_ohe(), cat_cols),
        ],
        remainder="drop"
    )
    rf = RandomForestRegressor(
        n_estimators=400,
        random_state=random_state,
        n_jobs=-1
    )
    return Pipeline(steps=[("prep", prep), ("rf", rf)])

def get_feature_names_from_pipeline(pipeline, num_cols, cat_cols):
    prep = pipeline.named_steps["prep"]
    try:
        feat_names = list(prep.get_feature_names_out())
        feat_names = [fn.replace("num__", "").replace("cat__", "") for fn in feat_names]
        return feat_names
    except Exception:
        # å…œåº•ï¼šå°è¯•æ‰‹åŠ¨ç»„åˆ
        names = []
        names.extend(num_cols)
        try:
            ohe = None
            for name, trans, cols in prep.transformers_:
                if name == "cat":
                    ohe = trans
                    break
            if hasattr(ohe, "get_feature_names_out"):
                names.extend(list(ohe.get_feature_names_out(cat_cols)))
            else:
                # æ— æ³•å®‰å…¨è·å–ï¼Œå¡«å ä½
                rf = pipeline.named_steps["rf"]
                k = max(0, rf.n_features_in_ - len(num_cols))
                names.extend([f"{cat_cols[0]}_oh_{i}" for i in range(k)])
        except Exception:
            pass
        return names

# =================== æ•°æ®è¯»å–ä¸ç‰¹å¾å‡†å¤‡ ===================
roads = gpd.read_file(ROADS_PATH)
csv   = pd.read_csv(CSV_PATH)

# åˆå¹¶ AADT
id_roads, id_csv = find_id_key(roads, csv)
aadt_col         = find_aadt_col(csv)
roads["_join_id"] = roads[id_roads].astype(str).str.strip()
csv["_join_id"]   = csv[id_csv].astype(str).str.strip()
csv_agg = (csv.dropna(subset=[aadt_col])
              .groupby("_join_id", as_index=False)[aadt_col].mean())
roads = roads.merge(csv_agg, on="_join_id", how="left")
roads = roads.rename(columns={aadt_col: "aadt_obs"})

# æ•°å€¼/ç±»åˆ«ç‰¹å¾åˆ—
if "population" not in roads.columns:
    roads["population"] = 0.0
roads["maxspeed_num"] = numeric_maxspeed(roads["maxspeed"]) if "maxspeed" in roads.columns else np.nan

roadtype_col = None
for cand in ["roadtype", "type", "highway", "type_level"]:
    if cand in roads.columns:
        roadtype_col = cand
        break
if roadtype_col is None:
    roadtype_col = "roadtype_fallback"
    roads[roadtype_col] = "unknown"

roaddens_col = None
for cand in ["road_density", "road_dens", "density"]:
    if cand in roads.columns:
        roaddens_col = cand
        break
if roaddens_col is None:
    roaddens_col = "road_density"
    roads[roaddens_col] = 0.0

poi_cols = choose_poi_columns(roads)
num_cols = ["population", "maxspeed_num", roaddens_col] + poi_cols
# å»é‡åŒæ—¶ä¿æŒé¡ºåº
num_cols = [c for c in dict.fromkeys(num_cols) if c in roads.columns]
cat_cols = [roadtype_col]

# ä»…ç”¨å¸¦ AADT çš„æ ·æœ¬
labeled = roads[~roads["aadt_obs"].isna()].copy()
if len(labeled) < 30:
    raise ValueError(f"å¸¦ AADT çš„æ ·æœ¬å¤ªå°‘ï¼ˆ{len(labeled)}ï¼‰ï¼Œæ— æ³•ç¨³å®šåœ°åšç©ºé—´åˆ’åˆ†ä¸è®­ç»ƒã€‚")

# =================== ç©ºé—´å‡åŒ€ 80/20 æ‹†åˆ† ===================
labeled_m = labeled.to_crs(epsg=CRS_METRIC).copy()
coords = np.vstack([labeled_m.geometry.centroid.x.values,
                    labeled_m.geometry.centroid.y.values]).T

N = len(labeled_m)
n_clusters = int(np.clip(np.sqrt(N), 10, 80))  # ~sqrt(N)ï¼›[10,80] çº¦æŸ
kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED, n_init="auto")
cluster_id = kmeans.fit_predict(coords)
labeled["cluster_id"] = cluster_id

rng = np.random.RandomState(RANDOM_SEED)
test_index = []
for cid, idxs in labeled.groupby("cluster_id").groups.items():
    idxs = np.array(list(idxs))
    n_in_cluster = len(idxs)
    if n_in_cluster <= 4:
        continue
    k = max(1, int(round(n_in_cluster * TEST_RATIO)))
    chosen = rng.choice(idxs, size=k, replace=False)
    test_index.extend(chosen.tolist())

test_index = np.array(sorted(set(test_index)))
train_index = np.array([i for i in labeled.index.values if i not in test_index])

# ç»„è£…è®­ç»ƒ/æµ‹è¯•ç‰¹å¾
Xtr = roads.loc[train_index, num_cols + cat_cols].copy()
ytr = roads.loc[train_index, "aadt_obs"].astype(float).values
Xte = roads.loc[test_index,  num_cols + cat_cols].copy()
yte = roads.loc[test_index,  "aadt_obs"].astype(float).values

# ç¼ºå¤±å¤„ç†
for df in (Xtr, Xte):
    for c in num_cols: df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
    for c in cat_cols: df[c] = df[c].astype(str).fillna("unknown")

# =================== è¿­ä»£ï¼šéšæœºæœç´¢ + é‡è¦æ€§è£å‰ª ===================
def run_iterative_rf(
    Xtr, ytr, Xte, yte, groups,
    init_num_cols, init_cat_cols,
    max_rounds=MAX_ROUNDS,
    keep_cum=KEEP_CUM_IMPORTANCE,
    min_imp=MIN_IMPORTANCE,
    early_stop=EARLY_STOP_ROUNDS,
    n_rs=N_RANDOM_SEARCH
):
    sel_num = list(init_num_cols)
    sel_cat = list(init_cat_cols)

    gkf = GroupKFold(n_splits=min(5, len(np.unique(groups))))

    best_reg = None
    best_score = -np.inf
    best_cols = (sel_num[:], sel_cat[:])
    no_improve = 0
    history = []

    for rnd in range(1, max_rounds + 1):
        pipe = make_reg_pipeline(sel_num, sel_cat, RANDOM_SEED)
        param_distributions = {
            "rf__n_estimators": randint(300, 900),
            "rf__max_depth": randint(6, 32),
            "rf__min_samples_split": randint(2, 10),
            "rf__min_samples_leaf": randint(1, 8),
            "rf__max_features": uniform(0.3, 0.7),  # 0.3~1.0
            "rf__bootstrap": [True, False],
        }
        rs = RandomizedSearchCV(
            pipe, param_distributions=param_distributions,
            n_iter=n_rs, cv=gkf.split(Xtr[sel_num + sel_cat], ytr, groups),
            scoring="r2", n_jobs=-1, random_state=RANDOM_SEED, verbose=0
        )
        rs.fit(Xtr[sel_num + sel_cat], ytr)

        cand_reg = rs.best_estimator_
        cand_reg.fit(Xtr[sel_num + sel_cat], ytr)

        yhat_tr = cand_reg.predict(Xtr[sel_num + sel_cat])
        yhat_te = cand_reg.predict(Xte[sel_num + sel_cat])

        tr_r2  = r2_score(ytr, yhat_tr)
        tr_rmse= math.sqrt(mean_squared_error(ytr, yhat_tr))
        te_r2  = r2_score(yte, yhat_te)
        te_rmse= math.sqrt(mean_squared_error(yte, yhat_te))

        # è®°å½•å†å²
        history.append({
            "round": rnd,
            "n_num": len(sel_num), "n_cat": len(sel_cat),
            "cv_r2": rs.best_score_,
            "train_r2": tr_r2, "train_rmse": tr_rmse,
            "test_r2": te_r2,  "test_rmse": te_rmse,
            "best_params": rs.best_params_
        })

        # ä¿å­˜æœ¬è½®å†…ç½®é‡è¦æ€§
        rf = cand_reg.named_steps["rf"]
        feat_names = get_feature_names_from_pipeline(cand_reg, sel_num, sel_cat)
        importances = rf.feature_importances_
        imp_df = pd.DataFrame({"feature": feat_names, "importance": importances})\
                    .sort_values("importance", ascending=False).reset_index(drop=True)
        imp_df.to_csv(f"feature_importance_round{rnd}.csv", index=False, encoding="utf-8-sig")

        # æ˜¯å¦åˆ·æ–°æœ€ä¼˜
        improved = te_r2 > best_score + 1e-4
        if improved:
            best_reg = cand_reg
            best_score = te_r2
            best_cols = (sel_num[:], sel_cat[:])
            no_improve = 0
        else:
            no_improve += 1

        # åŸºäºç´¯è®¡é‡è¦æ€§è£å‰ªï¼Œå¾—åˆ°ä¸‹ä¸€è½®ç‰¹å¾é›†åˆ
        imp_df["cum"] = imp_df["importance"].cumsum()
        keep_mask = (imp_df["cum"] <= keep_cum) | (imp_df["importance"] >= min_imp)
        kept_feats = imp_df.loc[keep_mask, "feature"].tolist()

        # æ•°å€¼åˆ—ç›´æ¥åŒ¹é…ï¼›ç±»åˆ«åˆ—ç­–ç•¥ï¼šåªè¦å‡ºç°ä»»æ„ä¸€ä¸ª one-hot åˆ—ï¼Œå°±ä¿ç•™åŸå§‹ç±»åˆ«åˆ—
        new_sel_num = [c in kept_feats and c for c in sel_num]
        new_sel_num = [c for c in new_sel_num if c]
        keep_cat = any([(c == f) or f.startswith(sel_cat[0]) for f in kept_feats]) if sel_cat else False
        new_sel_cat = sel_cat if keep_cat else []

        # å…œåº•ï¼šé¿å…å…¨åˆ ç©º
        if len(new_sel_num) + len(new_sel_cat) == 0:
            fallback = sel_num[:1] if sel_num else []
            new_sel_num = fallback

        sel_num, sel_cat = new_sel_num, new_sel_cat

        # æ—©åœ
        if no_improve >= early_stop:
            break

    hist_df = pd.DataFrame(history)
    hist_df.to_csv("rf_iter_history.csv", index=False, encoding="utf-8-sig")
    return best_reg, best_cols, hist_df

# ä»¥ç©ºé—´ç°‡ä¸º GroupKFold çš„åˆ†ç»„
# groups = roads.loc[train_index, "cluster_id"].values
groups = labeled.loc[train_index, "cluster_id"].values


best_reg, (best_num_cols, best_cat_cols), hist_df = run_iterative_rf(
    Xtr, ytr, Xte, yte, groups,
    init_num_cols=num_cols, init_cat_cols=cat_cols,
    max_rounds=MAX_ROUNDS, keep_cum=KEEP_CUM_IMPORTANCE,
    min_imp=MIN_IMPORTANCE, early_stop=EARLY_STOP_ROUNDS, n_rs=N_RANDOM_SEARCH
)

# =================== æœ€ä¼˜æ¨¡å‹è¯„ä¼°ä¸é‡è¦æ€§è¾“å‡º ===================
print("\n================ è¿­ä»£ç»“æŸï¼šæœ€ä¼˜æ¨¡å‹è¯„ä¼°ï¼ˆä»…RFï¼‰ ================")
yhat_tr_best = best_reg.predict(Xtr[best_num_cols + best_cat_cols])
yhat_te_best = best_reg.predict(Xte[best_num_cols + best_cat_cols])

print(f"[RF]  RÂ²(train) = {r2_score(ytr, yhat_tr_best):.4f}  "
      f"RMSE(train) = {math.sqrt(mean_squared_error(ytr, yhat_tr_best)):.1f}")
print(f"[RF]  RÂ²(test ) = {r2_score(yte, yhat_te_best):.4f}  "
      f"RMSE(test ) = {math.sqrt(mean_squared_error(yte, yhat_te_best)):.1f}")

rf_final = best_reg.named_steps["rf"]
feat_names_final = get_feature_names_from_pipeline(best_reg, best_num_cols, best_cat_cols)
imp_final = pd.DataFrame({"feature": feat_names_final, "importance": rf_final.feature_importances_})\
              .sort_values("importance", ascending=False).reset_index(drop=True)
imp_final.to_csv("feature_importance_best.csv", index=False, encoding="utf-8-sig")
print("ğŸ“„ å·²ä¿å­˜ï¼šrf_iter_history.csv, feature_importance_best.csv ä»¥åŠå„è½®çš„ feature_importance_round*.csv")

# å°è¯•ï¼šæµ‹è¯•é›†ç½®æ¢é‡è¦æ€§ï¼ˆæ›´ç¨³å¥ï¼‰
try:
    perm = permutation_importance(
        best_reg,
        Xte[best_num_cols + best_cat_cols],
        yte,
        n_repeats=N_PERM_REPEATS,
        random_state=RANDOM_SEED,
        n_jobs=-1,
        scoring="r2"
    )
    perm_df = pd.DataFrame({
        "feature": feat_names_final,
        "perm_importance_mean": perm.importances_mean,
        "perm_importance_std":  perm.importances_std
    }).sort_values("perm_importance_mean", ascending=False)
    perm_df.to_csv("feature_importance_permutation.csv", index=False, encoding="utf-8-sig")
    print("ğŸ“„ å·²ä¿å­˜ï¼šfeature_importance_permutation.csv")
except Exception as e:
    print(f"[WARN] ç½®æ¢é‡è¦æ€§å¤±è´¥ï¼š{e}")
