# svr_regression_kriging_with_saves.py
# ------------------------------------------------------------
# è¾“å…¥ï¼š
#   roads_with_poi_feats.geojson â€”â€” å…¨ç½‘é“è·¯ï¼ˆå« population/maxspeed/roadtype/road_density/POI ç‰¹å¾ï¼‰ï¼Œæ—  AADT
#   osm_id_with_aadt.csv         â€”â€” éƒ¨åˆ†é“è·¯çš„ ID + AADT å€¼ï¼ˆå¦‚ osm_id + aadtï¼‰
# è¿‡ç¨‹ï¼š
#   1) ç©ºé—´å‡åŒ€ 80/20 åˆ‡åˆ†ï¼ˆKMeans ç©ºé—´ç°‡ + åˆ†ç°‡æŠ½æ ·ï¼‰
#   2) SVR(RBF) å›å½’ï¼ˆæ ‡å‡†åŒ– + GroupKFold ç©ºé—´åˆ†ç»„ç½‘æ ¼æœç´¢ï¼‰â†’ è®­ç»ƒæ®‹å·®
#   3) æ®‹å·®çš„â€œé“è·¯ç½‘ç»œæœ€çŸ­è·¯å¾„è·ç¦»â€æ™®é€šå…‹é‡Œé‡‘ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼‰
#   4) åˆæˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹ï¼ˆå…¨ç½‘ï¼‰ï¼Œå¹¶è¯„ä¼° RÂ²/RMSEï¼ˆåŸºäºç©ºé—´å‡åŒ€æµ‹è¯•é›†ï¼‰
#   5) ä¿å­˜ï¼š
#       - roads_svr_pred.geojsonï¼ˆçº¯å›å½’é¢„æµ‹ï¼‰
#       - roads_rk_pred.geojsonï¼ˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹ï¼‰
#       - svr_gridcv_results.csvï¼ˆç½‘æ ¼æœç´¢è¯¦ç»†ç»“æœï¼‰
#       - svr_best_params.jsonï¼ˆæœ€ä¼˜è¶…å‚ï¼‰
#       - svr_pred_train_test.csvï¼ˆè®­ç»ƒ/æµ‹è¯• y_true vs y_predï¼‰
#       - feature_importance_detailed.csv / feature_importance_family.csvï¼ˆPermutation Importanceï¼‰
# ä¾èµ–ï¼šgeopandas shapely numpy pandas scikit-learn networkx scipy
# ------------------------------------------------------------

import warnings
warnings.filterwarnings("ignore")

import json, math
import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx

from shapely.geometry import LineString, MultiLineString, Point
from sklearn.cluster import KMeans
from sklearn.compose import ColumnTransformer
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, GroupKFold
from sklearn.inspection import permutation_importance
from scipy.spatial import cKDTree

# =================== æ‰‹åŠ¨å‚æ•°ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ ===================
ROADS_PATH = "roads_with_density.geojson"
CSV_PATH   = "osm_id_with_aadt.csv"
OUT_R_SVR  = "roads_svr_pred4.geojson"   # çº¯å›å½’é¢„æµ‹
OUT_RK     = "roads_rk_pred4.geojson"    # å›å½’å…‹é‡Œé‡‘é¢„æµ‹

CRS_METRIC  = 32647     # æ›¼è°· UTM 47Nï¼ˆç±³ï¼‰
TEST_RATIO  = 0.20
RANDOM_SEED = 42

# Kriging æŒ‡æ•°å˜å·®å‡½æ•°é»˜è®¤å‚æ•°ï¼ˆæ ·æœ¬ä¸è¶³ä»¥æ‹Ÿåˆæ—¶ä½¿ç”¨ï¼‰
DEFAULT_NUGGET = 0.05
DEFAULT_C      = 1.0
DEFAULT_A      = 1500.0  # ç›¸å…³ç¨‹ï¼ˆç±³ï¼‰
KRIGING_RIDGE  = 1e-6    # çº¿æ€§è§£ç¨³å®šé¡¹
# ==========================================================


# ------------------ å·¥å…·å‡½æ•° ------------------
def find_id_key(roads_df, csv_df):
    candidates = ["osm_id","osmid","id","road_id","roadid"]
    roads_cols = {c.lower(): c for c in roads_df.columns}
    csv_cols   = {c.lower(): c for c in csv_df.columns}
    for lc in candidates:
        if lc in roads_cols and lc in csv_cols:
            return roads_cols[lc], csv_cols[lc]
    inter = set(roads_cols.keys()) & set(csv_cols.keys())
    if inter:
        lc = list(inter)[0]
        return roads_cols[lc], csv_cols[lc]
    raise ValueError("æ‰¾ä¸åˆ°å…±åŒIDåˆ—ï¼›è¯·ç¡®ä¿ä¸¤ä»½æ•°æ®éƒ½æœ‰ç›¸åŒçš„ ID å­—æ®µï¼ˆå¦‚ osm_idï¼‰ã€‚")

def find_aadt_col(csv_df):
    for c in ["aadt","AADT","aadt_value","AADT_value","value"]:
        if c in csv_df.columns: return c
    return csv_df.columns[-1]

def numeric_maxspeed(series):
    s = series.astype(str).str.extract(r"(\d+\.?\d*)", expand=False)
    return pd.to_numeric(s, errors="coerce")

def choose_poi_columns(df):
    prefixes = ("front20_", "dens100_", "dens300_", "dens800_", "dist_", "entropy_")
    return [c for c in df.columns if c.startswith(prefixes)]

def build_graph_from_lines(gdf_metric):
    G = nx.Graph()
    def add_line(line: LineString):
        coords = list(line.coords)
        for u, v in zip(coords[:-1], coords[1:]):
            pu, pv = Point(u), Point(v)
            w = pu.distance(pv)
            if w <= 0: continue
            if not G.has_node(u): G.add_node(u, pos=u)
            if not G.has_node(v): G.add_node(v, pos=v)
            if G.has_edge(u, v): G[u][v]["weight"] = min(G[u][v]["weight"], w)
            else: G.add_edge(u, v, weight=w)
    for geom in gdf_metric.geometry:
        if geom is None or geom.is_empty: continue
        if isinstance(geom, LineString): add_line(geom)
        elif isinstance(geom, MultiLineString):
            for sub in geom.geoms: add_line(sub)
    return G

def kdtree_from_nodes(G):
    nodes = np.array([G.nodes[n]["pos"] for n in G.nodes])
    tree  = cKDTree(nodes)
    node_list = list(G.nodes)
    return tree, nodes, node_list

def snap_point_to_graph_node(point: Point, tree, nodes, node_list):
    d, idx = tree.query([point.x, point.y], k=1)
    return node_list[idx]

def network_distances_from_sources(G, source_nodes, target_nodes):
    target_set = set(target_nodes)
    dist_maps = []
    for s in source_nodes:
        lengths = nx.single_source_dijkstra_path_length(G, s, weight="weight")
        dist_maps.append({t: lengths.get(t, np.inf) for t in target_set})
    return dist_maps

def fit_simple_variogram(h, gam, resid_var):
    # æŒ‡æ•°æ¨¡å‹ gamma(h) = nugget + c*(1-exp(-h/a))
    try:
        from scipy.optimize import curve_fit
        ok = np.isfinite(h) & np.isfinite(gam) & (h < np.inf)
        if ok.sum() < 20:
            return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A
        def model(x, nugget, c, a): return nugget + c*(1.0 - np.exp(-x/a))
        p0 = [np.nanpercentile(gam[ok], 10),
              np.nanmax(gam[ok]) - np.nanpercentile(gam[ok], 10),
              np.nanpercentile(h[ok], 75)]
        popt, _ = curve_fit(model, h[ok], gam[ok], p0=p0, bounds=(0, [np.inf,np.inf,np.inf]), maxfev=20000)
        nugget, c, a = popt
        c = max(1e-6, min(resid_var, c))
        a = float(a) if np.isfinite(a) and a > 0 else DEFAULT_A
        return float(nugget), float(c), a
    except Exception:
        return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A

def cov_exp(h, nugget, c, a):
    return c * np.exp(-h / a)

def ordinary_kriging_weights(C, c_vec):
    n = C.shape[0]
    A = np.zeros((n+1, n+1), dtype=float)
    A[:n, :n] = C
    A[:n,  n] = 1.0
    A[ n, :n] = 1.0
    b = np.zeros(n+1, dtype=float)
    b[:n] = c_vec
    b[ n] = 1.0
    try:
        sol = np.linalg.solve(A, b)
    except np.linalg.LinAlgError:
        sol = np.linalg.lstsq(A, b, rcond=None)[0]
    return sol[:n]
# -----------------------------------------------------------


# 1) è¯»å…¥ä¸åˆå¹¶ AADT
roads = gpd.read_file(ROADS_PATH)
csv   = pd.read_csv(CSV_PATH)

id_roads, id_csv = find_id_key(roads, csv)
aadt_col         = find_aadt_col(csv)

roads["_join_id"] = roads[id_roads].astype(str).str.strip()
csv["_join_id"]   = csv[id_csv].astype(str).str.strip()

csv_agg = (csv.dropna(subset=[aadt_col])
              .groupby("_join_id", as_index=False)[aadt_col].mean())
roads = roads.merge(csv_agg, on="_join_id", how="left")
roads = roads.rename(columns={aadt_col: "aadt_obs"})

# 2) ç‰¹å¾å‡†å¤‡
if "population" not in roads.columns:
    roads["population"] = 0.0
roads["maxspeed_num"] = numeric_maxspeed(roads["maxspeed"]) if "maxspeed" in roads.columns else np.nan

roadtype_col = next((c for c in ["roadtype","type","highway","type_level"] if c in roads.columns), None)
if roadtype_col is None:
    roadtype_col = "roadtype_fallback"
    roads[roadtype_col] = "unknown"

roaddens_col = next((c for c in ["road_density","road_dens","density"] if c in roads.columns), None)
if roaddens_col is None:
    roaddens_col = "road_density"
    roads[roaddens_col] = 0.0

poi_cols = choose_poi_columns(roads)
num_cols = ["population", "maxspeed_num", roaddens_col] + poi_cols
num_cols = [c for c in dict.fromkeys(num_cols) if c in roads.columns]
cat_cols = [roadtype_col]

# 3) ä»…ç”¨â€œæœ‰ AADTâ€çš„é“è·¯åšå¸¦æ ‡æ³¨é›†ï¼›ç©ºé—´å‡åŒ€ 80/20 åˆ‡åˆ†
labeled = roads[~roads["aadt_obs"].isna()].copy()
if len(labeled) < 30:
    raise ValueError(f"å¸¦ AADT çš„æ ·æœ¬å¤ªå°‘ï¼ˆ{len(labeled)}ï¼‰ï¼Œä¸è¶³ä»¥åšç¨³å®šçš„ç©ºé—´æ‹†åˆ†ã€‚")

labeled_m = labeled.to_crs(epsg=CRS_METRIC).copy()
coords = np.vstack([labeled_m.geometry.centroid.x.values,
                    labeled_m.geometry.centroid.y.values]).T

N = len(labeled_m)
n_clusters = int(np.clip(np.sqrt(N), 5, min(80, N)))
kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED, n_init=10)
cluster_id = kmeans.fit_predict(coords)
labeled["cluster_id"] = cluster_id

# åˆ†ç°‡æŠ½æ ·ï¼šæ¯ç°‡çº¦ 20% ä¸ºæµ‹è¯•
rng = np.random.RandomState(RANDOM_SEED)
test_index = []
for cid, idxs in labeled.groupby("cluster_id").groups.items():
    idxs = np.array(list(idxs))
    m = len(idxs)
    if m <= 4:  # å°ç°‡å…¨è¿›è®­ç»ƒ
        continue
    k = max(1, int(round(m * TEST_RATIO)))
    chosen = rng.choice(idxs, size=k, replace=False)
    test_index.extend(chosen.tolist())
test_index = np.array(sorted(set(test_index)))
train_index = np.array([i for i in labeled.index.values if i not in test_index])

# è®­ç»ƒ/æµ‹è¯•è¡¨
Xtr = roads.loc[train_index, num_cols + cat_cols].copy()
ytr = roads.loc[train_index, "aadt_obs"].astype(float).values
Xte = roads.loc[test_index,  num_cols + cat_cols].copy()
yte = roads.loc[test_index,  "aadt_obs"].astype(float).values
for df in (Xtr, Xte):
    for c in num_cols: df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
    df[roadtype_col] = df[roadtype_col].astype(str).fillna("unknown")

# 4) SVR å›å½’ï¼ˆRBFï¼‰+ ç©ºé—´åˆ†ç»„ç½‘æ ¼æœç´¢ï¼ˆGroupKFoldï¼‰
prep = ColumnTransformer(
    transformers=[
        ("num", "passthrough", num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
    ],
    remainder="drop"
)
pipe = Pipeline([
    ("prep",  prep),
    ("scale", StandardScaler(with_mean=True, with_std=True)),
    ("svr",   SVR(kernel="rbf"))
])

param_grid = {
    "svr__C": [3, 10, 30, 100],
    "svr__epsilon": [0.05, 0.1, 0.3],
    "svr__gamma": ["scale", 0.1, 0.03, 0.01],
}
groups = labeled.loc[train_index, "cluster_id"].values  # åªç”¨è®­ç»ƒé›†ç°‡åš CV åˆ†ç»„
gkf = GroupKFold(n_splits=5)
gs = GridSearchCV(pipe, param_grid=param_grid, cv=gkf.split(Xtr, ytr, groups),
                  scoring="r2", n_jobs=-1, verbose=1)
gs.fit(Xtr, ytr)

best = gs.best_estimator_
print("Best params:", gs.best_params_, "CV R2:", gs.best_score_)

# è®­ç»ƒ/æµ‹è¯•é›†æ€§èƒ½ï¼ˆçº¯å›å½’ï¼‰
yhat_tr = best.predict(Xtr)
yhat_te = best.predict(Xte)
res_tr  = ytr - yhat_tr

print(f"[å›å½’] R2(train)={r2_score(ytr, yhat_tr):.3f}  RMSE(train)={math.sqrt(mean_squared_error(ytr, yhat_tr)):.1f}")
print(f"[å›å½’] R2(test )={r2_score(yte, yhat_te):.3f}  RMSE(test )={math.sqrt(mean_squared_error(yte, yhat_te)):.1f}")

# 5) ç½‘ç»œè·ç¦» OKï¼šä»…ç”¨è®­ç»ƒé›†æ„å»ºå…‹é‡Œé‡‘ç³»ç»Ÿ
roads_m = roads.to_crs(epsg=CRS_METRIC)
print("[Graph] æ„å»ºè·¯ç½‘å›¾â€¦")
G = build_graph_from_lines(roads_m)
print(f"[Graph] èŠ‚ç‚¹={G.number_of_nodes()} è¾¹={G.number_of_edges()}")

train_pts = roads_m.loc[train_index, "geometry"].centroid
all_pts   = roads_m.geometry.centroid

tree, nodes_arr, node_list = kdtree_from_nodes(G)
train_nodes = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in train_pts]
all_nodes   = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in all_pts]

# åˆå¹¶åŒèŠ‚ç‚¹è®­ç»ƒæ ·æœ¬ï¼ˆæ®‹å·®å–å‡å€¼ï¼‰
from collections import defaultdict
node_to_resids = defaultdict(list)
for n, rval in zip(train_nodes, res_tr.astype(float)):
    node_to_resids[n].append(rval)

uniq_train_nodes = list(node_to_resids.keys())
r = np.array([np.mean(node_to_resids[n]) for n in uniq_train_nodes], dtype=float)

# è·ç¦»çŸ©é˜µ
dist_maps_tt = network_distances_from_sources(G, uniq_train_nodes, uniq_train_nodes)
D_tt = np.array([[dist_maps_tt[i][n_j] for n_j in uniq_train_nodes] for i in range(len(uniq_train_nodes))], dtype=float)

dist_maps_tp = network_distances_from_sources(G, uniq_train_nodes, all_nodes)
D_tp = np.array([[dist_maps_tp[i][n_j] for n_j in all_nodes] for i in range(len(uniq_train_nodes))], dtype=float)

# 6) åŠå˜å¼‚æ‹Ÿåˆï¼ˆç®€åŒ–æŠ½æ ·ï¼‰
pairs = []
Nw = len(r)
max_pairs = min(5000, Nw*(Nw-1)//2)
step = max(1, (Nw*(Nw-1)//2)//max_pairs)
for i in range(Nw):
    for j in range(i+1, Nw, step):
        pairs.append((i, j))
pairs = pairs[:max_pairs]
h   = np.array([D_tt[i,j] for (i,j) in pairs], dtype=float)
gam = 0.5 * (r[[i for i,_ in pairs]] - r[[j for _,j in pairs]])**2

nugget, c, a = fit_simple_variogram(h, gam, resid_var=np.var(r))
print(f"[Variogram] nugget={nugget:.4f}  c={c:.4f}  a={a:.1f} m")

# 7) æ®‹å·® OK é¢„æµ‹ï¼ˆå¯¹å…¨ç½‘ï¼‰
C = cov_exp(D_tt, nugget, c, a)
np.fill_diagonal(C, c + nugget)
C = C + np.eye(C.shape[0]) * KRIGING_RIDGE
C_targets = cov_exp(D_tp, nugget, c, a)

def krige_one(c_vec, C_train, resid_train):
    w = ordinary_kriging_weights(C_train, c_vec)
    return float(np.dot(w, resid_train))

print("[Kriging] å¯¹å…¨éƒ¨é“è·¯åšæ®‹å·® OK é¢„æµ‹â€¦")
rk_resid_all = np.zeros(C_targets.shape[1], dtype=float)
for j in range(C_targets.shape[1]):
    rk_resid_all[j] = krige_one(C_targets[:, j], C, r)

# 8) åˆæˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹ï¼ˆå…¨ç½‘ï¼‰ï¼Œå¹¶è¯„ä¼°ï¼ˆç”¨ç©ºé—´å‡åŒ€æµ‹è¯•é›†ï¼‰
# çº¯å›å½’å…¨ç½‘é¢„æµ‹
X_all = roads[num_cols + [roadtype_col]].copy()
for c in num_cols: X_all[c] = pd.to_numeric(X_all[c], errors="coerce").fillna(0.0)
X_all[roadtype_col] = X_all[roadtype_col].astype(str).fillna("unknown")
yhat_all = best.predict(X_all)

# è¾“å‡ºè¡¨
roads_out_reg = roads.copy()
roads_out_reg["aadt_pred_reg_svr"] = yhat_all.astype(float)
roads_out_reg.to_file(OUT_R_SVR, driver="GeoJSON")
print(f"âœ… å·²è¾“å‡ºï¼š{OUT_R_SVR}ï¼ˆçº¯å›å½’é¢„æµ‹ï¼‰")

roads_out_rk = roads.copy()
roads_out_rk["aadt_obs"]      = roads_out_rk["aadt_obs"].astype(float)
roads_out_rk["aadt_pred_reg"] = yhat_all.astype(float)
roads_out_rk["rk_resid"]      = rk_resid_all.astype(float)
roads_out_rk["aadt_pred_rk"]  = (roads_out_rk["aadt_pred_reg"] + roads_out_rk["rk_resid"]).astype(float)
roads_out_rk.to_file(OUT_RK, driver="GeoJSON")
print(f"âœ… å·²è¾“å‡ºï¼š{OUT_RK}ï¼ˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹ï¼‰")

# è¯„ä¼°ï¼ˆä»…æµ‹è¯•é›†ï¼‰
mask_all_is_test = roads.index.isin(test_index)
rk_pred_test = roads_out_rk.loc[mask_all_is_test, "aadt_pred_rk"].values
print(f"[RK]  R2(test)  = {r2_score(yte, rk_pred_test):.3f}")
print(f"[RK]  RMSE(test)= {math.sqrt(mean_squared_error(yte, rk_pred_test)):.1f}")

# 9) ä¿å­˜ç½‘æ ¼æœç´¢ä¸è®­ç»ƒ/æµ‹è¯•é¢„æµ‹å¯¹æ¯”
pd.DataFrame(gs.cv_results_).to_csv("svr_gridcv_results.csv", index=False, encoding="utf-8-sig")
with open("svr_best_params.json", "w", encoding="utf-8") as f:
    json.dump(gs.best_params_, f, ensure_ascii=False, indent=2)

pd.DataFrame({
    "set": (["train"]*len(ytr)) + (["test"]*len(yte)),
    "_join_id": list(roads.loc[train_index, "_join_id"].values) + list(roads.loc[test_index, "_join_id"].values),
    "y_true": np.r_[ytr, yte],
    "y_pred_reg": np.r_[yhat_tr, yhat_te],
    "y_pred_rk":  np.r_[yhat_tr + rk_resid_all[roads.index.isin(train_index)],
                        rk_pred_test]
}).to_csv("svr_pred_train_test.csv", index=False, encoding="utf-8-sig")
print("ğŸ“„ å·²ä¿å­˜ï¼šsvr_gridcv_results.csv, svr_best_params.json, svr_pred_train_test.csv")

# 10) Permutation Importanceï¼ˆåŸºäºæµ‹è¯•é›†ï¼Œç²’åº¦åˆ°å±•å¼€ç‰¹å¾ï¼‰
prep_best  = best.named_steps["prep"]
scale_best = best.named_steps["scale"]
svr_best   = best.named_steps["svr"]

feat_names = prep_best.get_feature_names_out()
feat_names = [fn.replace("num__", "").replace("cat__", "") for fn in feat_names]

# åœ¨â€œscale+svrâ€ä¸Šåš permutation importanceï¼ˆè¾“å…¥ä¸º prep è¾“å‡ºï¼‰
Xte_design = prep_best.transform(Xte)
Xte_df = pd.DataFrame(Xte_design, columns=feat_names)

from sklearn.base import clone
svr_est = Pipeline([("scale", scale_best), ("svr", svr_best)])
pi = permutation_importance(svr_est, Xte_df, yte, scoring="r2",
                            n_repeats=5, random_state=RANDOM_SEED, n_jobs=-1)

feat_import_df = (pd.DataFrame({
    "feature": feat_names,
    "importance_mean": pi.importances_mean,
    "importance_std":  pi.importances_std
}).sort_values("importance_mean", ascending=False).reset_index(drop=True))

# å®¶æ—æ±‡æ€»
def family_of(f):
    if f.startswith(("dens100_","dens300_","dens800_")): return "poi_density"
    if f.startswith("front20_"): return "poi_frontage"
    if f.startswith("dist_"):    return "poi_distance"
    if f.startswith("entropy_"): return "poi_entropy"
    if f.startswith(("roadtype","highway","type")): return "roadtype(onehot)"
    if f == "population":      return "population"
    if f == "maxspeed_num":    return "maxspeed"
    if f == roaddens_col:      return "road_density"
    return "other"

feat_import_df["family"] = feat_import_df["feature"].map(family_of)
family_import = (feat_import_df.groupby("family", as_index=False)["importance_mean"].sum()
                                .sort_values("importance_mean", ascending=False))

feat_import_df.to_csv("feature_importance_detailed4.csv", index=False, encoding="utf-8-sig")
family_import.to_csv("feature_importance_family4.csv", index=False, encoding="utf-8-sig")
print("ğŸ“„ å·²ä¿å­˜ï¼šfeature_importance_detailed.csv, feature_importance_family.csv")

print("\n================ æ€»ç»“ ================")
print(f"[å›å½’]   RÂ²(train)={r2_score(ytr,yhat_tr):.4f}  RÂ²(test)={r2_score(yte,yhat_te):.4f}")
print(f"[RK]     RÂ²(test) ={r2_score(yte,rk_pred_test):.4f}")
print("è¾“å‡ºï¼š")
print(f" - çº¯å›å½’é¢„æµ‹ï¼š{OUT_R_SVR}")
print(f" - å›å½’å…‹é‡Œé‡‘ï¼š{OUT_RK}")
print(" - ç½‘æ ¼æœç´¢ç»“æœï¼šsvr_gridcv_results.csv / svr_best_params.json")
print(" - è®­ç»ƒ/æµ‹è¯•å¯¹æ¯”ï¼šsvr_pred_train_test.csv")
print(" - ç‰¹å¾é‡è¦æ€§ï¼šfeature_importance_detailed.csv / feature_importance_family.csv")
