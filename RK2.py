
# regression_kriging_geo_stratified_split.py
# ------------------------------------------------------------
# 1) è¯»å…¥ï¼šroads_with_poi_feats.geojsonï¼ˆå…¨ç½‘ç‰¹å¾ï¼‰ï¼Œosm_id_with_aadt.csvï¼ˆID + AADTï¼‰
# 2) ç©ºé—´å‡åŒ€ 80/20 æ‹†åˆ†ï¼ˆKMeans ç©ºé—´ç°‡ + åˆ†ç°‡æŠ½æ ·ï¼‰
# 3) å›å½’ï¼ˆå« one-hotã€ç¼ºå¤±å¤„ç†ï¼‰â†’ è®­ç»ƒæ®‹å·®
# 4) åŸºäºé“è·¯ç½‘ç»œæœ€çŸ­è·¯å¾„è·ç¦»çš„æ™®é€šå…‹é‡Œé‡‘ï¼ˆåªç”¨è®­ç»ƒé›†ï¼‰
# 5) åˆæˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹ã€è¯„ä¼°ä¸å˜é‡é‡è¦æ€§
# è¾“å‡ºï¼šroads_rk_pred.geojsonã€feature_importance_detailed.csvã€feature_importance_family.csv
# ------------------------------------------------------------

import warnings
warnings.filterwarnings("ignore")

import math
import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx

from shapely.geometry import LineString, MultiLineString, Point
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.cluster import KMeans
from scipy.spatial import cKDTree

# =================== é…ç½® ===================
ROADS_PATH = "roads_with_density.geojson"
CSV_PATH   = "osm_id_with_aadt.csv"
OUT_PATH   = "roads_rk_pred.geojson"

CRS_METRIC  = 32647     # æ›¼è°· UTM 47Nï¼ˆç±³ï¼‰
TEST_RATIO  = 0.20
RANDOM_SEED = 42

# åŠå˜å¼‚å‡½æ•°ï¼ˆæŒ‡æ•°ï¼‰é»˜è®¤å‚æ•°ï¼ˆæ ·æœ¬ä¸è¶³ä»¥æ‹Ÿåˆæ—¶ä½¿ç”¨ï¼‰
DEFAULT_NUGGET = 0.05
DEFAULT_C      = 1.0    # ä¼šä¹˜ä»¥æ®‹å·®æ–¹å·®
DEFAULT_A      = 1500.0 # ç›¸å…³ç¨‹ï¼ˆç±³ï¼‰
RIDGE          = 1e-6   # æ•°å€¼ç¨³å®šé¡¹

# ============== å·¥å…·å‡½æ•° ==============
def find_id_key(roads_df, csv_df):
    candidates = ["osm_id", "osmid", "id", "road_id", "roadid"]
    roads_cols = {c.lower(): c for c in roads_df.columns}
    csv_cols   = {c.lower(): c for c in csv_df.columns}
    for lc in candidates:
        if lc in roads_cols and lc in csv_cols:
            return roads_cols[lc], csv_cols[lc]
    inter = set(roads_cols.keys()) & set(csv_cols.keys())
    if inter:
        lc = list(inter)[0]
        return roads_cols[lc], csv_cols[lc]
    raise ValueError("æ‰¾ä¸åˆ°å…±åŒIDåˆ—ï¼›è¯·ç¡®ä¿ roads ä¸ csv éƒ½åŒ…å«ç›¸åŒçš„ ID å­—æ®µï¼ˆå¦‚ osm_idï¼‰ã€‚")

def find_aadt_col(csv_df):
    for c in ["aadt","AADT","aadt_value","AADT_value","value"]:
        if c in csv_df.columns: return c
    return csv_df.columns[-1]

def numeric_maxspeed(series):
    s = series.astype(str).str.extract(r"(\d+\.?\d*)", expand=False)
    return pd.to_numeric(s, errors="coerce")

def choose_poi_columns(df):
    prefixes = ("front20_", "dens100_", "dens300_", "dens800_", "dist_", "entropy_")
    return [c for c in df.columns if c.startswith(prefixes)]

def build_graph_from_lines(gdf_metric):
    G = nx.Graph()
    def add_line(line: LineString):
        coords = list(line.coords)
        for u, v in zip(coords[:-1], coords[1:]):
            pu, pv = Point(u), Point(v)
            w = pu.distance(pv)
            if w <= 0: continue
            if not G.has_node(u): G.add_node(u, pos=u)
            if not G.has_node(v): G.add_node(v, pos=v)
            if G.has_edge(u, v): G[u][v]["weight"] = min(G[u][v]["weight"], w)
            else: G.add_edge(u, v, weight=w)
    for geom in gdf_metric.geometry:
        if geom is None or geom.is_empty: continue
        if isinstance(geom, LineString): add_line(geom)
        elif isinstance(geom, MultiLineString):
            for sub in geom.geoms: add_line(sub)
    return G

def kdtree_from_nodes(G):
    nodes = np.array([G.nodes[n]["pos"] for n in G.nodes])
    tree  = cKDTree(nodes)
    node_list = list(G.nodes)
    return tree, nodes, node_list

def snap_point_to_graph_node(point: Point, tree, nodes, node_list):
    d, idx = tree.query([point.x, point.y], k=1)
    return node_list[idx]

def network_distances_from_sources(G, source_nodes, target_nodes):
    target_set = set(target_nodes)
    dist_maps = []
    for s in source_nodes:
        lengths = nx.single_source_dijkstra_path_length(G, s, weight="weight")
        dist_maps.append({t: lengths.get(t, np.inf) for t in target_set})
    return dist_maps

def fit_simple_variogram(h, gam, resid_var):
    # æŒ‡æ•°æ¨¡å‹ gamma(h) = nugget + c*(1-exp(-h/a))
    try:
        from scipy.optimize import curve_fit
        ok = np.isfinite(h) & np.isfinite(gam) & (h < np.inf)
        if ok.sum() < 20:
            return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A
        def model(x, nugget, c, a): return nugget + c*(1.0 - np.exp(-x/a))
        p0 = [np.nanpercentile(gam[ok], 10),
              np.nanmax(gam[ok]) - np.nanpercentile(gam[ok], 10),
              np.nanpercentile(h[ok], 75)]
        popt, _ = curve_fit(model, h[ok], gam[ok], p0=p0, bounds=(0, [np.inf,np.inf,np.inf]), maxfev=10000)
        nugget, c, a = popt
        c = max(1e-6, min(resid_var, c))
        a = float(a) if np.isfinite(a) and a > 0 else DEFAULT_A
        return float(nugget), float(c), a
    except Exception:
        return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A

def cov_exp(h, nugget, c, a):
    return c * np.exp(-h / a)

def ordinary_kriging_weights(C, c_vec):
    n = C.shape[0]
    A = np.zeros((n+1, n+1), dtype=float)
    A[:n, :n] = C
    A[:n,  n] = 1.0
    A[ n, :n] = 1.0
    b = np.zeros(n+1, dtype=float)
    b[:n] = c_vec
    b[ n] = 1.0
    try:
        sol = np.linalg.solve(A, b)
    except np.linalg.LinAlgError:
        sol = np.linalg.lstsq(A, b, rcond=None)[0]
    return sol[:n]

# =================== ä¸»æµç¨‹ ===================
# 1) è¯»å–æ•°æ®å¹¶åˆå¹¶ AADT
roads = gpd.read_file(ROADS_PATH)
csv   = pd.read_csv(CSV_PATH)

id_roads, id_csv = find_id_key(roads, csv)
aadt_col         = find_aadt_col(csv)

roads["_join_id"] = roads[id_roads].astype(str).str.strip()
csv["_join_id"]   = csv[id_csv].astype(str).str.strip()

csv_agg = (csv.dropna(subset=[aadt_col])
              .groupby("_join_id", as_index=False)[aadt_col].mean())
roads = roads.merge(csv_agg, on="_join_id", how="left")
roads = roads.rename(columns={aadt_col: "aadt_obs"})

# 2) ç‰¹å¾å‡†å¤‡
# population
if "population" not in roads.columns:
    roads["population"] = 0.0

# maxspeed æ•°å€¼åŒ–
roads["maxspeed_num"] = numeric_maxspeed(roads["maxspeed"]) if "maxspeed" in roads.columns else np.nan

# roadtypeï¼ˆç±»åˆ« one-hotï¼‰
roadtype_col = None
for cand in ["roadtype", "type", "highway", "type_level"]:
    if cand in roads.columns:
        roadtype_col = cand; break
if roadtype_col is None:
    roadtype_col = "roadtype_fallback"
    roads[roadtype_col] = "unknown"

# road_density
roaddens_col = None
for cand in ["road_density", "road_dens", "density"]:
    if cand in roads.columns:
        roaddens_col = cand; break
if roaddens_col is None:
    roaddens_col = "road_density"
    roads[roaddens_col] = 0.0

# POI ç‰¹å¾
poi_cols = choose_poi_columns(roads)

num_cols = ["population", "maxspeed_num", roaddens_col] + poi_cols
num_cols = [c for c in dict.fromkeys(num_cols) if c in roads.columns]
cat_cols = [roadtype_col]

# 3) ä»…ç”¨â€œæœ‰ AADTâ€çš„é“è·¯åšå¸¦æ ‡æ³¨é›†
labeled = roads[~roads["aadt_obs"].isna()].copy()
if len(labeled) < 30:
    raise ValueError(f"å¸¦ AADT çš„æ ·æœ¬å¤ªå°‘ï¼ˆ{len(labeled)}ï¼‰ï¼Œæ— æ³•ç¨³å®šåœ°åšç©ºé—´åˆ’åˆ†ä¸è®­ç»ƒã€‚")

# 3.1 ç©ºé—´å‡åŒ€ 80/20 æ‹†åˆ†ï¼ˆKMeans ç©ºé—´ç°‡ + åˆ†ç°‡æŠ½æ ·ï¼‰
# åœ¨ç±³åˆ¶æŠ•å½±ä¸‹å–è´¨å¿ƒåæ ‡ç”¨äºèšç±»
labeled_m = labeled.to_crs(epsg=CRS_METRIC).copy()
coords = np.vstack([labeled_m.geometry.centroid.x.values,
                    labeled_m.geometry.centroid.y.values]).T

# è‡ªé€‚åº”ç¡®å®šç°‡æ•°ï¼š~sqrt(N)ï¼Œé™åˆ¶åœ¨ [10, 80]
N = len(labeled_m)
n_clusters = int(np.clip(np.sqrt(N), 10, 80))
kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED, n_init="auto")
cluster_id = kmeans.fit_predict(coords)
labeled["cluster_id"] = cluster_id

# åœ¨æ¯ä¸ªç°‡å†…æŠ½å–çº¦ 20% ä½œä¸ºæµ‹è¯•ï¼ˆè‡³å°‘ 1 ä¸ªï¼Œç°‡å¾ˆå°æ—¶å¯è·³è¿‡ï¼‰
rng = np.random.RandomState(RANDOM_SEED)
test_index = []
for cid, idxs in labeled.groupby("cluster_id").groups.items():
    idxs = np.array(list(idxs))
    n_in_cluster = len(idxs)
    if n_in_cluster <= 4:
        # ç°‡å¤ªå°ï¼Œå…¨éƒ¨è¿›è®­ç»ƒï¼Œæé«˜ç¨³å®šæ€§
        continue
    k = max(1, int(round(n_in_cluster * TEST_RATIO)))
    chosen = rng.choice(idxs, size=k, replace=False)
    test_index.extend(chosen.tolist())

test_index = np.array(sorted(set(test_index)))
train_index = np.array([i for i in labeled.index.values if i not in test_index])

# 3.2 ç»„è£…è®­ç»ƒ/æµ‹è¯•ç‰¹å¾
Xtr = roads.loc[train_index, num_cols + cat_cols].copy()
ytr = roads.loc[train_index, "aadt_obs"].astype(float).values
Xte = roads.loc[test_index,  num_cols + cat_cols].copy()
yte = roads.loc[test_index,  "aadt_obs"].astype(float).values

# ç¼ºå¤±å¤„ç†
for df in (Xtr, Xte):
    for c in num_cols: df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
    df[roadtype_col] = df[roadtype_col].astype(str).fillna("unknown")

# 4) å›å½’æ¨¡å‹
reg = Pipeline(steps=[
    ("prep", ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
        ],
        remainder="drop"
    )),
    ("rf", RandomForestRegressor(n_estimators=400, random_state=RANDOM_SEED, n_jobs=-1))
])
reg.fit(Xtr, ytr)

yhat_tr = reg.predict(Xtr)
yhat_te = reg.predict(Xte)
res_tr  = ytr - yhat_tr

print(f"[å›å½’] R2(train)={r2_score(ytr, yhat_tr):.3f}  RMSE(train)={math.sqrt(mean_squared_error(ytr, yhat_tr)):.1f}")
print(f"[å›å½’] R2(test )={r2_score(yte, yhat_te):.3f}  RMSE(test )={math.sqrt(mean_squared_error(yte, yhat_te)):.1f}")

# 5) ç½‘ç»œè·ç¦» OKï¼šä»…ç”¨è®­ç»ƒé›†æ„å»ºå…‹é‡Œé‡‘ç³»ç»Ÿ
roads_m = roads.to_crs(epsg=CRS_METRIC)
print("[Graph] æ„å»ºè·¯ç½‘å›¾â€¦")
G = build_graph_from_lines(roads_m)
print(f"[Graph] èŠ‚ç‚¹={G.number_of_nodes()} è¾¹={G.number_of_edges()}")

# è®­ç»ƒä¸å…¨éƒ¨çš„ä»£è¡¨ç‚¹ï¼ˆè´¨å¿ƒï¼‰â†’ snap åˆ°æœ€è¿‘èŠ‚ç‚¹
train_pts = roads_m.loc[train_index, "geometry"].centroid
all_pts   = roads_m.geometry.centroid

tree, nodes_arr, node_list = kdtree_from_nodes(G)
train_nodes = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in train_pts]
all_nodes   = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in all_pts]

# åˆå¹¶åŒä¸€èŠ‚ç‚¹ä¸Šçš„è®­ç»ƒæ ·æœ¬ï¼ˆå–æ®‹å·®å‡å€¼ï¼‰ï¼Œä¿è¯ç»´åº¦ä¸€è‡´ä¸çŸ©é˜µç¨³å®š
from collections import defaultdict
node_to_resids = defaultdict(list)
for n, rval in zip(train_nodes, res_tr.astype(float)):
    node_to_resids[n].append(rval)

uniq_train_nodes = list(node_to_resids.keys())
r = np.array([np.mean(node_to_resids[n]) for n in uniq_train_nodes], dtype=float)

# è®­ç»ƒâ†’è®­ç»ƒç½‘ç»œè·ç¦»çŸ©é˜µ
dist_maps_tt = network_distances_from_sources(G, uniq_train_nodes, uniq_train_nodes)
D_tt = np.array([[dist_maps_tt[i][n_j] for n_j in uniq_train_nodes] for i in range(len(uniq_train_nodes))], dtype=float)

# è®­ç»ƒâ†’å…¨éƒ¨ç½‘ç»œè·ç¦»çŸ©é˜µ
dist_maps_tp = network_distances_from_sources(G, uniq_train_nodes, all_nodes)
D_tp = np.array([[dist_maps_tp[i][n_j] for n_j in all_nodes] for i in range(len(uniq_train_nodes))], dtype=float)

# 6) åŠå˜å¼‚å‡½æ•°æ‹Ÿåˆï¼ˆç®€åŒ–ï¼‰
# ç»éªŒåŠå˜å¼‚ï¼šgamma(h) = 0.5 (ri - rj)^2ï¼Œç¨€ç–æŠ½æ ·
pairs = []
Nw = len(r)
max_pairs = min(5000, Nw*(Nw-1)//2)
step = max(1, (Nw*(Nw-1)//2)//max_pairs)
for i in range(Nw):
    for j in range(i+1, Nw, step):
        pairs.append((i, j))
pairs = pairs[:max_pairs]

h   = np.array([D_tt[i,j] for (i,j) in pairs], dtype=float)
gam = 0.5 * (r[[i for i,_ in pairs]] - r[[j for _,j in pairs]])**2

nugget, c, a = fit_simple_variogram(h, gam, resid_var=np.var(r))
print(f"[Variogram] nugget={nugget:.4f}  c={c:.4f}  a={a:.1f} m")

# 7) åæ–¹å·®çŸ©é˜µä¸ç›®æ ‡åæ–¹å·®
C = cov_exp(D_tt, nugget, c, a)
np.fill_diagonal(C, c + nugget)
C = C + np.eye(C.shape[0])*RIDGE

C_targets = cov_exp(D_tp, nugget, c, a)   # å½¢çŠ¶ï¼š(n_train_unique, n_all)

def krige_one(c_vec, C_train, resid_train):
    w = ordinary_kriging_weights(C_train, c_vec)
    return float(np.dot(w, resid_train))

print("[Kriging] å¯¹å…¨éƒ¨é“è·¯åšæ®‹å·® OK é¢„æµ‹â€¦")
rk_resid_all = np.zeros(C_targets.shape[1], dtype=float)
for j in range(C_targets.shape[1]):
    rk_resid_all[j] = krige_one(C_targets[:, j], C, r)

# 8) åˆæˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹å¹¶è¯„ä¼°ï¼ˆç”¨ç©ºé—´å‡åŒ€çš„æµ‹è¯•é›†ï¼‰
# å›å½’å…¨ç½‘é¢„æµ‹
X_all = roads[num_cols + [roadtype_col]].copy()
for c in num_cols: X_all[c] = pd.to_numeric(X_all[c], errors="coerce").fillna(0.0)
X_all[roadtype_col] = X_all[roadtype_col].astype(str).fillna("unknown")
yhat_all = reg.predict(X_all)

roads_out = roads.copy()
roads_out["aadt_obs"]      = roads_out["aadt_obs"].astype(float)
roads_out["aadt_pred_reg"] = yhat_all.astype(float)
roads_out["rk_resid"]      = rk_resid_all.astype(float)
roads_out["aadt_pred_rk"]  = (roads_out["aadt_pred_reg"] + roads_out["rk_resid"]).astype(float)

# è¯„ä¼°ï¼ˆä»…é’ˆå¯¹æµ‹è¯•é›†çš„è§‚æµ‹ï¼‰
mask_all_is_test = roads.index.isin(test_index)
rk_pred_test = roads_out.loc[mask_all_is_test, "aadt_pred_rk"].values

print(f"[RK]  R2(test)  = {r2_score(yte, rk_pred_test):.3f}")
print(f"[RK]  RMSE(test)= {math.sqrt(mean_squared_error(yte, rk_pred_test)):.1f}")

# 9) è¾“å‡ºä¸»ç»“æœ
roads_out.to_file(OUT_PATH, driver="GeoJSON")
print(f"âœ… å·²è¾“å‡ºï¼š{OUT_PATH}")

# 10) å˜é‡é‡è¦æ€§ & RÂ² æ±‡æ€»
print("\n================ å˜é‡é‡è¦æ€§ & RÂ² æ±‡æ€» ================")
reg_r2_train = r2_score(ytr, yhat_tr)
reg_r2_test  = r2_score(yte, yhat_te)
rk_r2_test   = r2_score(yte, rk_pred_test)
print(f"[å›å½’]   RÂ²(train) = {reg_r2_train:.4f}")
print(f"[å›å½’]   RÂ²(test ) = {reg_r2_test :.4f}")
print(f"[å›å½’å…‹é‡Œé‡‘] RÂ²(test ) = {rk_r2_test  :.4f}")

rf   = reg.named_steps["rf"]
prep = reg.named_steps["prep"]

# æ‹¿åˆ—å
try:
    feat_names = prep.get_feature_names_out()
    feat_names = [fn.replace("num__", "").replace("cat__", "") for fn in feat_names]
except Exception:
    feat_names = []
    feat_names.extend(num_cols)
    try:
        ohe = None
        for name, trans, cols in prep.transformers_:
            if name == "cat": ohe = trans; break
        if hasattr(ohe, "get_feature_names_out"):
            feat_names.extend(list(ohe.get_feature_names_out([roadtype_col])))
        else:
            feat_names.extend([f"{roadtype_col}_<onehot_{i}>" for i in range(rf.n_features_in_ - len(num_cols))])
    except Exception:
        pass

importances = rf.feature_importances_
feat_import_df = pd.DataFrame({"feature": feat_names, "importance": importances})
feat_import_df = feat_import_df.sort_values("importance", ascending=False).reset_index(drop=True)

print("\n[Top 30 Feature Importance]")
print(feat_import_df.head(30).to_string(index=False))

def family_of(f):
    if f.startswith(("dens100_","dens300_","dens800_")): return "poi_density"
    if f.startswith("front20_"): return "poi_frontage"
    if f.startswith("dist_"):    return "poi_distance"
    if f.startswith("entropy_"): return "poi_entropy"
    if f.startswith(roadtype_col): return "roadtype(onehot)"
    if f == "population":      return "population"
    if f == "maxspeed_num":    return "maxspeed"
    if f == roaddens_col:      return "road_density"
    return "other"

feat_import_df["family"] = feat_import_df["feature"].map(family_of)
family_import = (feat_import_df.groupby("family", as_index=False)["importance"].sum()
                                .sort_values("importance", ascending=False))

feat_import_df.to_csv("feature_importance_detailed.csv", index=False, encoding="utf-8-sig")
family_import.to_csv("feature_importance_family.csv", index=False, encoding="utf-8-sig")

print("\n[Feature Importance by Family]")
print(family_import.to_string(index=False))
print("\nğŸ“„ å·²ä¿å­˜ï¼šfeature_importance_detailed.csv, feature_importance_family.csv")
print("=========================================================\n")
