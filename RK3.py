# regression_kriging_geo_stratified_linear.py
# ------------------------------------------------------------
# è¾“å…¥ï¼š
#   roads_with_poi_feats.geojson â€”â€” å…¨ç½‘é“è·¯ï¼ˆå« population/maxspeed/roadtype/road_density/POI ç‰¹å¾ï¼‰ï¼Œæ—  AADT
#   osm_id_with_aadt.csv         â€”â€” éƒ¨åˆ†é“è·¯çš„ ID + AADT å€¼ï¼ˆå¦‚ osm_id + aadtï¼‰
# ä¸»è¦è¿‡ç¨‹ï¼š
#   1) ç©ºé—´å‡åŒ€ 80/20 åˆ’åˆ†ï¼ˆKMeans ç©ºé—´ç°‡ + åˆ†ç°‡æŠ½æ ·ï¼‰
#   2) çº¿æ€§æ¨¡å‹ï¼ˆRidgeï¼‰å›å½’ â†’ è®­ç»ƒæ®‹å·®
#   3) æ®‹å·®çš„â€œç½‘ç»œè·ç¦»â€æ™®é€šå…‹é‡Œé‡‘ï¼ˆä»…ç”¨è®­ç»ƒé›†æ„å»ºï¼‰
#   4) åˆæˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹ã€è¯„ä¼°ä¸å˜é‡é‡è¦æ€§ï¼ˆçº¿æ€§ç³»æ•°ï¼‰
# è¾“å‡ºï¼š
#   roads_rk_pred.geojson
#   feature_coefficients_detailed.csv
#   feature_coefficients_family.csv
# ------------------------------------------------------------

import warnings
warnings.filterwarnings("ignore")

import math
import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx

from shapely.geometry import LineString, MultiLineString, Point
from sklearn.cluster import KMeans
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from scipy.spatial import cKDTree

# =================== åŸºæœ¬é…ç½® ===================
ROADS_PATH = "roads_with_density.geojson"
CSV_PATH   = "osm_id_with_aadt.csv"
OUT_PATH   = "roads_rk_pred2.geojson"

CRS_METRIC  = 32647     # æ›¼è°· UTM 47Nï¼ˆç±³ï¼‰
TEST_RATIO  = 0.20
RANDOM_SEED = 42

# åŠå˜å¼‚å‡½æ•°ï¼ˆæŒ‡æ•°ï¼‰é»˜è®¤å‚æ•°ï¼ˆæ ·æœ¬ä¸è¶³ä»¥æ‹Ÿåˆæ—¶ä½¿ç”¨ï¼‰
DEFAULT_NUGGET = 0.05
DEFAULT_C      = 1.0    # ä¼šä¸æ®‹å·®æ–¹å·®ç»“åˆ
DEFAULT_A      = 1500.0 # ç›¸å…³ç¨‹ï¼ˆç±³ï¼‰
RIDGE_EPS      = 1e-6   # å…‹é‡Œé‡‘çº¿æ€§è§£ç¨³å®šé¡¹

# =================== å·¥å…·å‡½æ•° ===================
def find_id_key(roads_df, csv_df):
    candidates = ["osm_id", "osmid", "id", "road_id", "roadid"]
    roads_cols = {c.lower(): c for c in roads_df.columns}
    csv_cols   = {c.lower(): c for c in csv_df.columns}
    for lc in candidates:
        if lc in roads_cols and lc in csv_cols:
            return roads_cols[lc], csv_cols[lc]
    inter = set(roads_cols.keys()) & set(csv_cols.keys())
    if inter:
        lc = list(inter)[0]
        return roads_cols[lc], csv_cols[lc]
    raise ValueError("æ‰¾ä¸åˆ°å…±åŒIDåˆ—ï¼›è¯·ç¡®ä¿ä¸¤ä»½æ•°æ®éƒ½æœ‰ç›¸åŒçš„ ID å­—æ®µï¼ˆå¦‚ osm_idï¼‰ã€‚")

def find_aadt_col(csv_df):
    for c in ["aadt","AADT","aadt_value","AADT_value","value"]:
        if c in csv_df.columns: return c
    return csv_df.columns[-1]

def numeric_maxspeed(series):
    s = series.astype(str).str.extract(r"(\d+\.?\d*)", expand=False)
    return pd.to_numeric(s, errors="coerce")

def choose_poi_columns(df):
    prefixes = ("front20_", "dens100_", "dens300_", "dens800_", "dist_", "entropy_")
    return [c for c in df.columns if c.startswith(prefixes)]

def build_graph_from_lines(gdf_metric):
    G = nx.Graph()
    def add_line(line: LineString):
        coords = list(line.coords)
        for u, v in zip(coords[:-1], coords[1:]):
            pu, pv = Point(u), Point(v)
            w = pu.distance(pv)
            if w <= 0: continue
            if not G.has_node(u): G.add_node(u, pos=u)
            if not G.has_node(v): G.add_node(v, pos=v)
            if G.has_edge(u, v): G[u][v]["weight"] = min(G[u][v]["weight"], w)
            else: G.add_edge(u, v, weight=w)
    for geom in gdf_metric.geometry:
        if geom is None or geom.is_empty: continue
        if isinstance(geom, LineString): add_line(geom)
        elif isinstance(geom, MultiLineString):
            for sub in geom.geoms: add_line(sub)
    return G

def kdtree_from_nodes(G):
    nodes = np.array([G.nodes[n]["pos"] for n in G.nodes])
    tree  = cKDTree(nodes)
    node_list = list(G.nodes)
    return tree, nodes, node_list

def snap_point_to_graph_node(point: Point, tree, nodes, node_list):
    d, idx = tree.query([point.x, point.y], k=1)
    return node_list[idx]

def network_distances_from_sources(G, source_nodes, target_nodes):
    target_set = set(target_nodes)
    dist_maps = []
    for s in source_nodes:
        lengths = nx.single_source_dijkstra_path_length(G, s, weight="weight")
        dist_maps.append({t: lengths.get(t, np.inf) for t in target_set})
    return dist_maps

def fit_simple_variogram(h, gam, resid_var):
    # æŒ‡æ•°æ¨¡å‹ gamma(h) = nugget + c*(1-exp(-h/a))
    try:
        from scipy.optimize import curve_fit
        ok = np.isfinite(h) & np.isfinite(gam) & (h < np.inf)
        if ok.sum() < 20:
            return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A
        def model(x, nugget, c, a): return nugget + c*(1.0 - np.exp(-x/a))
        p0 = [np.nanpercentile(gam[ok], 10),
              np.nanmax(gam[ok]) - np.nanpercentile(gam[ok], 10),
              np.nanpercentile(h[ok], 75)]
        popt, _ = curve_fit(model, h[ok], gam[ok], p0=p0, bounds=(0, [np.inf,np.inf,np.inf]), maxfev=20000)
        nugget, c, a = popt
        c = max(1e-6, min(resid_var, c))
        a = float(a) if np.isfinite(a) and a > 0 else DEFAULT_A
        return float(nugget), float(c), a
    except Exception:
        return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A

def cov_exp(h, nugget, c, a):
    return c * np.exp(-h / a)

def ordinary_kriging_weights(C, c_vec):
    n = C.shape[0]
    A = np.zeros((n+1, n+1), dtype=float)
    A[:n, :n] = C
    A[:n,  n] = 1.0
    A[ n, :n] = 1.0
    b = np.zeros(n+1, dtype=float)
    b[:n] = c_vec
    b[ n] = 1.0
    try:
        sol = np.linalg.solve(A, b)
    except np.linalg.LinAlgError:
        sol = np.linalg.lstsq(A, b, rcond=None)[0]
    return sol[:n]

# =================== ä¸»æµç¨‹ ===================
# 1) è¯»å…¥ä¸åˆå¹¶ AADT
roads = gpd.read_file(ROADS_PATH)
csv   = pd.read_csv(CSV_PATH)

id_roads, id_csv = find_id_key(roads, csv)
aadt_col         = find_aadt_col(csv)

roads["_join_id"] = roads[id_roads].astype(str).str.strip()
csv["_join_id"]   = csv[id_csv].astype(str).str.strip()

csv_agg = (csv.dropna(subset=[aadt_col])
              .groupby("_join_id", as_index=False)[aadt_col].mean())
roads = roads.merge(csv_agg, on="_join_id", how="left")
roads = roads.rename(columns={aadt_col: "aadt_obs"})

# 2) ç‰¹å¾å‡†å¤‡
if "population" not in roads.columns:
    roads["population"] = 0.0

# maxspeed æ•°å€¼åŒ–
roads["maxspeed_num"] = numeric_maxspeed(roads["maxspeed"]) if "maxspeed" in roads.columns else np.nan

# roadtypeï¼ˆç±»åˆ«ï¼‰
roadtype_col = None
for cand in ["roadtype", "type", "highway", "type_level"]:
    if cand in roads.columns: roadtype_col = cand; break
if roadtype_col is None:
    roadtype_col = "roadtype_fallback"
    roads[roadtype_col] = "unknown"

# road_density
roaddens_col = None
for cand in ["road_density", "road_dens", "density"]:
    if cand in roads.columns: roaddens_col = cand; break
if roaddens_col is None:
    roaddens_col = "road_density"
    roads[roaddens_col] = 0.0

# POI ç‰¹å¾è‡ªåŠ¨è¯†åˆ«
poi_cols = choose_poi_columns(roads)

num_cols = ["population", "maxspeed_num", roaddens_col] + poi_cols
num_cols = [c for c in dict.fromkeys(num_cols) if c in roads.columns]  # å»é‡å¹¶ä¿å­˜åœ¨
cat_cols = [roadtype_col]

# 3) ä»…ç”¨â€œæœ‰ AADTâ€çš„é“è·¯åšå¸¦æ ‡æ³¨é›†ï¼›ç©ºé—´å‡åŒ€ 80/20 åˆ‡åˆ†
labeled = roads[~roads["aadt_obs"].isna()].copy()
if len(labeled) < 30:
    raise ValueError(f"å¸¦ AADT çš„æ ·æœ¬å¤ªå°‘ï¼ˆ{len(labeled)}ï¼‰ï¼Œä¸è¶³ä»¥åšç¨³å®šçš„ç©ºé—´æ‹†åˆ†ã€‚")

# åœ¨ç±³åˆ¶åæ ‡ä¸‹æŒ‰è´¨å¿ƒèšç±»
labeled_m = labeled.to_crs(epsg=CRS_METRIC).copy()
coords = np.vstack([labeled_m.geometry.centroid.x.values,
                    labeled_m.geometry.centroid.y.values]).T

N = len(labeled_m)
if N < 5:
    n_clusters = N
else:
    n_clusters = int(np.clip(np.sqrt(N), 5, min(80, N)))
kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED, n_init=10)
cluster_id = kmeans.fit_predict(coords)
labeled["cluster_id"] = cluster_id

# åˆ†ç°‡æŠ½æ ·ï¼šæ¯ç°‡çº¦ 20% ä¸ºæµ‹è¯•
rng = np.random.RandomState(RANDOM_SEED)
test_index = []
for cid, idxs in labeled.groupby("cluster_id").groups.items():
    idxs = np.array(list(idxs))
    m = len(idxs)
    if m <= 4:
        continue  # ç°‡å¤ªå°ï¼Œå…¨è¿›è®­ç»ƒ
    k = max(1, int(round(m * TEST_RATIO)))
    chosen = rng.choice(idxs, size=k, replace=False)
    test_index.extend(chosen.tolist())

test_index = np.array(sorted(set(test_index)))
train_index = np.array([i for i in labeled.index.values if i not in test_index])

# ç»„è£…è®­ç»ƒ/æµ‹è¯•ç‰¹å¾
Xtr = roads.loc[train_index, num_cols + cat_cols].copy()
ytr = roads.loc[train_index, "aadt_obs"].astype(float).values
Xte = roads.loc[test_index,  num_cols + cat_cols].copy()
yte = roads.loc[test_index,  "aadt_obs"].astype(float).values

# ç¼ºå¤±å¤„ç†
for df in (Xtr, Xte):
    for c in num_cols: df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
    df[roadtype_col] = df[roadtype_col].astype(str).fillna("unknown")

# 4) çº¿æ€§å›å½’ï¼ˆRidge æˆ– RidgeCVï¼‰
# ç”¨ RidgeCV è‡ªåŠ¨æŒ‘ alphaï¼›è‹¥æƒ³å›ºå®š alpha=1.0ï¼Œå¯æ”¹æˆ Ridge(alpha=1.0)
reg = Pipeline(steps=[
    ("prep", ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
        ],
        remainder="drop"
    )),
    ("scale", StandardScaler(with_mean=True, with_std=True)),
    ("lin", RidgeCV(alphas=[0.1, 0.3, 1, 3, 10, 30, 100]))
])
reg.fit(Xtr, ytr)

yhat_tr = reg.predict(Xtr)
yhat_te = reg.predict(Xte)
res_tr  = ytr - yhat_tr

print(f"[å›å½’] R2(train)={r2_score(ytr, yhat_tr):.3f}  RMSE(train)={math.sqrt(mean_squared_error(ytr, yhat_tr)):.1f}")
print(f"[å›å½’] R2(test )={r2_score(yte, yhat_te):.3f}  RMSE(test )={math.sqrt(mean_squared_error(yte, yhat_te)):.1f}")
try:
    print(f"[çº¿æ€§] RidgeCV é€‰æ‹©çš„ alpha = {reg.named_steps['lin'].alpha_}")
except Exception:
    pass

# 5) ç½‘ç»œè·ç¦» OKï¼šä»…ç”¨è®­ç»ƒé›†æ„å»ºå…‹é‡Œé‡‘ç³»ç»Ÿ
roads_m = roads.to_crs(epsg=CRS_METRIC)
print("[Graph] æ„å»ºè·¯ç½‘å›¾â€¦")
G = build_graph_from_lines(roads_m)
print(f"[Graph] èŠ‚ç‚¹={G.number_of_nodes()} è¾¹={G.number_of_edges()}")

train_pts = roads_m.loc[train_index, "geometry"].centroid
all_pts   = roads_m.geometry.centroid

tree, nodes_arr, node_list = kdtree_from_nodes(G)
train_nodes = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in train_pts]
all_nodes   = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in all_pts]

# åˆå¹¶åŒä¸€å›¾èŠ‚ç‚¹ä¸Šçš„è®­ç»ƒæ ·æœ¬ï¼šæ®‹å·®å–å‡å€¼ï¼Œé¿å…çŸ©é˜µç—…æ€ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´
from collections import defaultdict
node_to_resids = defaultdict(list)
for n, rval in zip(train_nodes, res_tr.astype(float)):
    node_to_resids[n].append(rval)

uniq_train_nodes = list(node_to_resids.keys())
r = np.array([np.mean(node_to_resids[n]) for n in uniq_train_nodes], dtype=float)

# è®­ç»ƒâ†’è®­ç»ƒç½‘ç»œè·ç¦»çŸ©é˜µ
dist_maps_tt = network_distances_from_sources(G, uniq_train_nodes, uniq_train_nodes)
D_tt = np.array([[dist_maps_tt[i][n_j] for n_j in uniq_train_nodes] for i in range(len(uniq_train_nodes))], dtype=float)

# è®­ç»ƒâ†’å…¨éƒ¨ç½‘ç»œè·ç¦»çŸ©é˜µ
dist_maps_tp = network_distances_from_sources(G, uniq_train_nodes, all_nodes)
D_tp = np.array([[dist_maps_tp[i][n_j] for n_j in all_nodes] for i in range(len(uniq_train_nodes))], dtype=float)

# 6) åŠå˜å¼‚æ‹Ÿåˆï¼ˆç®€åŒ–æŠ½æ ·ï¼‰
pairs = []
Nw = len(r)
max_pairs = min(5000, Nw*(Nw-1)//2)
step = max(1, (Nw*(Nw-1)//2)//max_pairs)
for i in range(Nw):
    for j in range(i+1, Nw, step):
        pairs.append((i, j))
pairs = pairs[:max_pairs]
h   = np.array([D_tt[i,j] for (i,j) in pairs], dtype=float)
gam = 0.5 * (r[[i for i,_ in pairs]] - r[[j for _,j in pairs]])**2

nugget, c, a = fit_simple_variogram(h, gam, resid_var=np.var(r))
print(f"[Variogram] nugget={nugget:.4f}  c={c:.4f}  a={a:.1f} m")

# 7) åæ–¹å·®çŸ©é˜µä¸ç›®æ ‡åæ–¹å·®
C = cov_exp(D_tt, nugget, c, a)
np.fill_diagonal(C, c + nugget)
C = C + np.eye(C.shape[0]) * RIDGE_EPS

C_targets = cov_exp(D_tp, nugget, c, a)   # (n_train_unique, n_all)

def krige_one(c_vec, C_train, resid_train):
    w = ordinary_kriging_weights(C_train, c_vec)
    return float(np.dot(w, resid_train))

print("[Kriging] å¯¹å…¨éƒ¨é“è·¯åšæ®‹å·® OK é¢„æµ‹â€¦")
rk_resid_all = np.zeros(C_targets.shape[1], dtype=float)
for j in range(C_targets.shape[1]):
    rk_resid_all[j] = krige_one(C_targets[:, j], C, r)

# 8) åˆæˆå›å½’å…‹é‡Œé‡‘é¢„æµ‹å¹¶è¯„ä¼°ï¼ˆç”¨ç©ºé—´å‡åŒ€çš„æµ‹è¯•é›†ï¼‰
# å›å½’å…¨ç½‘é¢„æµ‹
X_all = roads[num_cols + [roadtype_col]].copy()
for c in num_cols: X_all[c] = pd.to_numeric(X_all[c], errors="coerce").fillna(0.0)
X_all[roadtype_col] = X_all[roadtype_col].astype(str).fillna("unknown")
yhat_all = reg.predict(X_all)

roads_out = roads.copy()
roads_out["aadt_obs"]      = roads_out["aadt_obs"].astype(float)
roads_out["aadt_pred_reg"] = yhat_all.astype(float)
roads_out["rk_resid"]      = rk_resid_all.astype(float)
roads_out["aadt_pred_rk"]  = (roads_out["aadt_pred_reg"] + roads_out["rk_resid"]).astype(float)

# è¯„ä¼°ï¼ˆä»…æµ‹è¯•é›†ï¼‰
mask_all_is_test = roads.index.isin(test_index)
rk_pred_test = roads_out.loc[mask_all_is_test, "aadt_pred_rk"].values

print(f"[RK]  R2(test)  = {r2_score(yte, rk_pred_test):.3f}")
print(f"[RK]  RMSE(test)= {math.sqrt(mean_squared_error(yte, rk_pred_test)):.1f}")

# 9) å¯¼å‡ºä¸»ç»“æœ
roads_out.to_file(OUT_PATH, driver="GeoJSON")
print(f"âœ… å·²è¾“å‡ºï¼š{OUT_PATH}")

# 10) å˜é‡é‡è¦æ€§ï¼ˆçº¿æ€§ç³»æ•°ï¼‰ & RÂ² æ±‡æ€»
print("\n================ å˜é‡é‡è¦æ€§ & RÂ² æ±‡æ€» ================")
reg_r2_train = r2_score(ytr, yhat_tr)
reg_r2_test  = r2_score(yte, yhat_te)
rk_r2_test   = r2_score(yte, rk_pred_test)
print(f"[å›å½’]   RÂ²(train) = {reg_r2_train:.4f}")
print(f"[å›å½’]   RÂ²(test ) = {reg_r2_test :.4f}")
print(f"[å›å½’å…‹é‡Œé‡‘] RÂ²(test ) = {rk_r2_test  :.4f}")

lin  = reg.named_steps["lin"]
prep = reg.named_steps["prep"]

# å–å›å±•å¼€åçš„ç‰¹å¾å
try:
    feat_names = prep.get_feature_names_out()
    feat_names = [fn.replace("num__", "").replace("cat__", "") for fn in feat_names]
except Exception:
    feat_names = []
    feat_names.extend(num_cols)
    try:
        ohe = None
        for name, trans, cols in prep.transformers_:
            if name == "cat": ohe = trans; break
        if hasattr(ohe, "get_feature_names_out"):
            feat_names.extend(list(ohe.get_feature_names_out(cat_cols)))
        else:
            feat_names.extend([f"{cat_cols[0]}_onehot_{i}" for i in range(len(lin.coef_) - len(num_cols))])
    except Exception:
        pass

coefs = np.asarray(lin.coef_, dtype=float)
feat_import_df = pd.DataFrame({
    "feature": feat_names,
    "coef": coefs,
    "abs_coef": np.abs(coefs)
}).sort_values("abs_coef", ascending=False).reset_index(drop=True)

print("\n[Top 30 Linear Coefficients by |coef|]")
print(feat_import_df.head(30).to_string(index=False))

def family_of(f):
    if f.startswith(("dens100_","dens300_","dens800_")): return "poi_density"
    if f.startswith("front20_"): return "poi_frontage"
    if f.startswith("dist_"):    return "poi_distance"
    if f.startswith("entropy_"): return "poi_entropy"
    if f.startswith(roadtype_col): return "roadtype(onehot)"
    if f == "population":      return "population"
    if f == "maxspeed_num":    return "maxspeed"
    if f == roaddens_col:      return "road_density"
    return "other"

feat_import_df["family"] = feat_import_df["feature"].map(family_of)
family_import = (feat_import_df
                 .groupby("family", as_index=False)["abs_coef"].sum()
                 .rename(columns={"abs_coef":"abs_coef_sum"})
                 .sort_values("abs_coef_sum", ascending=False))

feat_import_df.to_csv("feature_coefficients_detailed2.csv", index=False, encoding="utf-8-sig")
family_import.to_csv("feature_coefficients_family2.csv", index=False, encoding="utf-8-sig")

print("\n[Coefficient Importance by Family] (sum |coef|)")
print(family_import.to_string(index=False))
print("\nğŸ“„ å·²ä¿å­˜ï¼šfeature_coefficients_detailed.csv, feature_coefficients_family.csv")
print("=========================================================\n")
