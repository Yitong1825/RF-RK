# regression_kriging_from_roads_and_csv.py
# ------------------------------------------------------------
# è¾“å…¥ï¼š
#   1) roads_with_poi_feats.geojson   â€”â€” å…¨éƒ¨é“è·¯ï¼ˆå« population/maxspeed/roadtype/road_density/POI ç‰¹å¾ï¼‰ï¼Œæ—  AADT
#   2) osm_id_with_aadt.csv           â€”â€” éƒ¨åˆ†é“è·¯çš„ ID + AADT å€¼ï¼ˆä¼˜å…ˆåˆ—åï¼šosm_id, aadtï¼‰
#
# è¾“å‡ºï¼š
#   roads_rk_pred.geojson â€”â€” åŒ…å« aadt_pred_regï¼ˆå›å½’ï¼‰ã€rk_residï¼ˆæ®‹å·®å…‹é‡Œé‡‘ï¼‰ã€aadt_pred_rkï¼ˆæœ€ç»ˆé¢„æµ‹ï¼‰
#                            ä»¥åŠ aadt_obsï¼ˆè‹¥è¯¥é“è·¯åœ¨CSVä¸­æœ‰è§‚æµ‹ï¼‰
# ------------------------------------------------------------

import warnings
warnings.filterwarnings("ignore")

import math
import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx

from shapely.geometry import LineString, MultiLineString, Point
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

# ============== è·¯å¾„é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ ==============
ROADS_PATH = "roads_with_poi_feats.geojson"
CSV_PATH   = "osm_id_with_aadt.csv"
OUT_PATH   = "roads_rk_pred.geojson"

# ============== æ¨¡å‹/æµç¨‹å‚æ•° ==============
RANDOM_SEED = 42
TEST_SIZE   = 0.2
CRS_METRIC  = 32647   # UTM 47Nï¼ˆæ›¼è°·ï¼‰

# åŠå˜å¼‚å‡½æ•°ï¼ˆæŒ‡æ•°ï¼‰é»˜è®¤å‚æ•°ï¼ˆå½“æ ·æœ¬ä¸è¶³ä»¥æ‹Ÿåˆæ—¶ä½¿ç”¨ï¼‰
DEFAULT_NUGGET = 0.05
DEFAULT_C      = 1.0    # ä¼šä¹˜ä»¥æ®‹å·®æ–¹å·®
DEFAULT_A      = 1500.0 # ç›¸å…³ç¨‹ aï¼ˆç±³é‡çº§ï¼ŒåŸå¸‚è·¯ç½‘å¸¸è§ 1-3kmï¼‰
RIDGE          = 1e-6   # çº¿æ€§ä»£æ•°ç¨³å®šé¡¹

# ============== å°å·¥å…·å‡½æ•° ==============
def find_id_key(roads_df, csv_df):
    """è‡ªåŠ¨å¯»æ‰¾ä¸¤è¾¹å…±åŒIDåˆ—ï¼ˆä¼˜å…ˆ osm_idï¼‰"""
    candidates = ["osm_id", "osmid", "id", "road_id", "roadid"]
    roads_cols = {c.lower(): c for c in roads_df.columns}
    csv_cols   = {c.lower(): c for c in csv_df.columns}
    for lc in candidates:
        if lc in roads_cols and lc in csv_cols:
            return roads_cols[lc], csv_cols[lc]
    # è‹¥æ²¡å‘½ä¸­ï¼Œå°è¯•ä»»æ„äº¤é›†åç›¸åŒçš„åˆ—
    inter = set(roads_cols.keys()) & set(csv_cols.keys())
    if inter:
        lc = list(inter)[0]
        return roads_cols[lc], csv_cols[lc]
    raise ValueError("æ‰¾ä¸åˆ°å…±åŒIDåˆ—ï¼›è¯·ç¡®ä¿ roads ä¸ csv éƒ½åŒ…å«ç›¸åŒçš„ ID å­—æ®µï¼ˆå¦‚ osm_idï¼‰ã€‚")

def find_aadt_col(csv_df):
    candidates = ["aadt", "AADT", "aadt_value", "AADT_value", "value"]
    for c in candidates:
        if c in csv_df.columns:
            return c
    # è‹¥å®Œå…¨æ‰¾ä¸åˆ°ï¼Œå°è¯•æœ€åä¸€åˆ—
    return csv_df.columns[-1]

def numeric_maxspeed(series):
    s = series.astype(str).str.extract(r"(\d+\.?\d*)", expand=False)
    return pd.to_numeric(s, errors="coerce")

def choose_poi_columns(df):
    prefixes = ("front20_", "dens100_", "dens300_", "dens800_", "dist_", "entropy_")
    cols = [c for c in df.columns if c.startswith(prefixes)]
    return cols

# æ„å»ºè·¯ç½‘å›¾
def build_graph_from_lines(gdf_metric):
    G = nx.Graph()
    def add_line(line: LineString):
        coords = list(line.coords)
        for u, v in zip(coords[:-1], coords[1:]):
            pu, pv = Point(u), Point(v)
            w = pu.distance(pv)
            if w <= 0:
                continue
            if not G.has_node(u): G.add_node(u, pos=u)
            if not G.has_node(v): G.add_node(v, pos=v)
            if G.has_edge(u, v):
                G[u][v]["weight"] = min(G[u][v]["weight"], w)
            else:
                G.add_edge(u, v, weight=w)

    for geom in gdf_metric.geometry:
        if geom is None or geom.is_empty: continue
        if isinstance(geom, LineString):
            add_line(geom)
        elif isinstance(geom, MultiLineString):
            for sub in geom.geoms:
                add_line(sub)
    return G

def kdtree_from_nodes(G):
    from scipy.spatial import cKDTree
    nodes = np.array([G.nodes[n]["pos"] for n in G.nodes])
    tree = cKDTree(nodes)
    node_list = list(G.nodes)
    return tree, nodes, node_list

def snap_point_to_graph_node(point: Point, tree, nodes, node_list):
    d, idx = tree.query([point.x, point.y], k=1)
    return node_list[idx]

def network_distances_from_sources(G, source_nodes, target_nodes):
    """ä»å¤šæºç‚¹åˆ°ä¸€ç»„ç›®æ ‡ç‚¹çš„ç½‘ç»œè·ç¦»ï¼ˆå­—å…¸åˆ—è¡¨ï¼‰"""
    target_set = set(target_nodes)
    dist_maps = []
    for s in source_nodes:
        lengths = nx.single_source_dijkstra_path_length(G, s, weight="weight")
        dist_maps.append({t: lengths.get(t, np.inf) for t in target_set})
    return dist_maps

# åŠå˜å¼‚æ‹Ÿåˆï¼ˆç®€åŒ–æŒ‡æ•°æ¨¡å‹ï¼‰ï¼šgamma(h)=nugget + c*(1-exp(-h/a))
def fit_simple_variogram(h, gam, resid_var):
    try:
        from scipy.optimize import curve_fit
        ok = np.isfinite(h) & np.isfinite(gam) & (h < np.inf)
        if ok.sum() < 20:
            return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A
        def model(x, nugget, c, a):
            return nugget + c * (1.0 - np.exp(-x / a))
        p0 = [np.nanpercentile(gam[ok], 10), np.nanmax(gam[ok]) - np.nanpercentile(gam[ok], 10), np.nanpercentile(h[ok], 75)]
        popt, _ = curve_fit(model, h[ok], gam[ok], p0=p0, bounds=(0, [np.inf, np.inf, np.inf]), maxfev=10000)
        nugget, c, a = popt
        c = max(1e-6, min(resid_var, c))
        a = float(a) if np.isfinite(a) and a > 0 else DEFAULT_A
        return float(nugget), float(c), a
    except Exception:
        return DEFAULT_NUGGET, max(1e-6, min(resid_var, DEFAULT_C*resid_var)), DEFAULT_A

def cov_exp(h, nugget, c, a):
    return c * np.exp(-h / a)

def ordinary_kriging_weights(C, c_vec):
    n = C.shape[0]
    A = np.zeros((n+1, n+1), dtype=float)
    A[:n, :n] = C
    A[:n,  n] = 1.0
    A[ n, :n] = 1.0
    b = np.zeros(n+1, dtype=float)
    b[:n] = c_vec
    b[ n] = 1.0
    try:
        sol = np.linalg.solve(A, b)
    except np.linalg.LinAlgError:
        sol = np.linalg.lstsq(A, b, rcond=None)[0]
    return sol[:n]

# ============== ä¸»æµç¨‹ ==============
# 1) è¯»å–æ•°æ®
roads = gpd.read_file(ROADS_PATH)
csv   = pd.read_csv(CSV_PATH)

# 2) è§„èŒƒ ID & åˆå¹¶ AADT
id_roads, id_csv = find_id_key(roads, csv)
aadt_col = find_aadt_col(csv)

# å°†ä¸¤è¾¹IDéƒ½è½¬ä¸ºå­—ç¬¦ä¸²ï¼ˆç¨³å¥åˆå¹¶ï¼‰
roads["_join_id"] = roads[id_roads].astype(str).str.strip()
csv["_join_id"]   = csv[id_csv].astype(str).str.strip()

# è‹¥ CSV æœ‰é‡å¤IDï¼Œåˆ™å¯¹ AADT æ±‚å‡å€¼ï¼ˆä½ ä¹Ÿå¯æ”¹æˆ first/medianï¼‰
csv_agg = (csv
           .dropna(subset=[aadt_col])
           .groupby("_join_id", as_index=False)[aadt_col].mean())

# åˆå¹¶ï¼šä¿ç•™æ‰€æœ‰é“è·¯ï¼Œé™„ä¸Šè§‚æµ‹ AADT
roads = roads.merge(csv_agg, on="_join_id", how="left", suffixes=("",""))
roads = roads.rename(columns={aadt_col: "aadt_obs"})

# 3) ç‰¹å¾å·¥ç¨‹ï¼šæŠ½å–ä½ æŒ‡å®šçš„ç‰¹å¾
# population
if "population" not in roads.columns:
    roads["population"] = 0.0

# maxspeed æ•°å€¼åŒ–
roads["maxspeed_num"] = numeric_maxspeed(roads["maxspeed"]) if "maxspeed" in roads.columns else np.nan

# roadtypeï¼ˆç±»åˆ« one-hotï¼‰
roadtype_col = None
for cand in ["roadtype", "type", "highway", "type_level"]:
    if cand in roads.columns:
        roadtype_col = cand
        break
if roadtype_col is None:
    roadtype_col = "roadtype_fallback"
    roads[roadtype_col] = "unknown"

# road_density
roaddens_col = None
for cand in ["road_density", "road_dens", "density"]:
    if cand in roads.columns:
        roaddens_col = cand
        break
if roaddens_col is None:
    roaddens_col = "road_density"
    roads[roaddens_col] = 0.0

# POI ç‰¹å¾è‡ªåŠ¨è¯†åˆ«
poi_cols = choose_poi_columns(roads)

# ç»„è£…ç‰¹å¾åˆ—
num_cols = ["population", "maxspeed_num", roaddens_col] + poi_cols
num_cols = [c for c in dict.fromkeys(num_cols) if c in roads.columns]  # å»é‡å¹¶ç¡®è®¤å­˜åœ¨
cat_cols = [roadtype_col]

# 4) è®­ç»ƒé›†ï¼ˆä»…æœ‰ aadt_obs çš„é“è·¯ï¼‰
labeled = roads[~roads["aadt_obs"].isna()].copy()
if len(labeled) < 20:
    raise ValueError(f"å¯ç”¨çš„ AADT è®­ç»ƒæ ·æœ¬è¿‡å°‘ï¼ˆ{len(labeled)} æ¡ï¼‰ã€‚è¯·æ£€æŸ¥ CSV åˆå¹¶æ˜¯å¦æˆåŠŸã€‚")

X = labeled[num_cols + cat_cols].copy()
y = labeled["aadt_obs"].astype(float).values

# ç¼ºå¤±å¤„ç†
for c in num_cols: X[c] = pd.to_numeric(X[c], errors="coerce").fillna(0.0)
X[roadtype_col] = X[roadtype_col].astype(str).fillna("unknown")

Xtr, Xte, ytr, yte, idxtr, idxte = train_test_split(
    X, y, labeled.index.values, test_size=TEST_SIZE, random_state=RANDOM_SEED
)

# 5) å›å½’å™¨ï¼ˆå¯æ›¿æ¢ä¸ºçº¿æ€§/å²­/SVRï¼‰
reg = Pipeline(steps=[
    ("prep", ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
        ],
        remainder="drop"
    )),
    ("rf", RandomForestRegressor(
        n_estimators=400, random_state=RANDOM_SEED, n_jobs=-1
    ))
])
reg.fit(Xtr, ytr)

yhat_tr = reg.predict(Xtr)
yhat_te = reg.predict(Xte)
res_tr  = ytr - yhat_tr
res_te  = yte - yhat_te

print(f"[å›å½’] R2(train)={r2_score(ytr, yhat_tr):.3f}  RMSE(train)={math.sqrt(mean_squared_error(ytr, yhat_tr)):.1f}")
print(f"[å›å½’] R2(test )={r2_score(yte, yhat_te):.3f}  RMSE(test )={math.sqrt(mean_squared_error(yte, yhat_te)):.1f}")

# 6) å»ºç«‹ç½‘ç»œå›¾å¹¶è®¡ç®—ç½‘ç»œè·ç¦»
roads_m = roads.to_crs(epsg=CRS_METRIC)
labeled_m = roads_m.loc[labeled.index]

print("[Graph] æ„å»ºè·¯ç½‘å›¾â€¦")
G = build_graph_from_lines(roads_m)
print(f"[Graph] èŠ‚ç‚¹={G.number_of_nodes()} è¾¹={G.number_of_edges()}")

# ç”¨å‡ ä½•è´¨å¿ƒä½œä¸ºä»£è¡¨ç‚¹ï¼Œsnap åˆ°æœ€è¿‘èŠ‚ç‚¹
train_pts = labeled_m.geometry.centroid
all_pts   = roads_m.geometry.centroid

from scipy.spatial import cKDTree
tree, nodes_arr, node_list = kdtree_from_nodes(G)
train_nodes = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in train_pts]
all_nodes   = [snap_point_to_graph_node(pt, tree, nodes_arr, node_list) for pt in all_pts]

print("[Graph] è®¡ç®—ç½‘ç»œè·ç¦»ï¼ˆè®­ç»ƒâ†’è®­ç»ƒ / è®­ç»ƒâ†’å…¨éƒ¨ï¼‰â€¦")
# è®­ç»ƒâ†’è®­ç»ƒ
dist_maps_tt = network_distances_from_sources(G, train_nodes, train_nodes)
D_tt = np.array([[dist_maps_tt[i][n_j] for n_j in train_nodes] for i in range(len(train_nodes))], dtype=float)

# è®­ç»ƒâ†’å…¨éƒ¨
dist_maps_tp = network_distances_from_sources(G, train_nodes, all_nodes)
D_tp = np.array([[dist_maps_tp[i][n_j] for n_j in all_nodes] for i in range(len(train_nodes))], dtype=float)

# 7) ç®€åŒ–åŠå˜å¼‚æ‹Ÿåˆ
r = res_tr.astype(float)
# æŠ½æ ·å¯¹
pairs = []
N = len(r)
max_pairs = min(5000, N*(N-1)//2)
step = max(1, (N*(N-1)//2)//max_pairs)
for i in range(N):
    for j in range(i+1, N, step):
        pairs.append((i, j))
pairs = pairs[:max_pairs]
h   = np.array([D_tt[i,j] for (i,j) in pairs], dtype=float)
gam = 0.5 * (r[[i for i,_ in pairs]] - r[[j for _,j in pairs]])**2

nugget, c, a = fit_simple_variogram(h, gam, resid_var=np.var(r))
print(f"[Variogram] nugget={nugget:.4f}  c={c:.4f}  a={a:.1f} m")

# 8) è®­ç»ƒåæ–¹å·®çŸ©é˜µ & ç›®æ ‡åæ–¹å·®å‘é‡
C = c * np.exp(-D_tt / a)
np.fill_diagonal(C, c + nugget)
C = C + np.eye(C.shape[0]) * RIDGE

C_targets = c * np.exp(-D_tp / a)  # (n_train, n_all)

def krige_one(c_vec, C_train, resid_train):
    # æ™®é€šå…‹é‡Œé‡‘æƒé‡
    w = ordinary_kriging_weights(C_train, c_vec)
    return float(np.dot(w, resid_train))

print("[Kriging] å¯¹å…¨éƒ¨é“è·¯åšæ®‹å·® OK é¢„æµ‹â€¦")
rk_resid_all = np.zeros(C_targets.shape[1], dtype=float)
for j in range(C_targets.shape[1]):
    rk_resid_all[j] = krige_one(C_targets[:, j], C, r)

# 9) ç»„åˆä¸ºå›å½’å…‹é‡Œé‡‘é¢„æµ‹ï¼Œå¹¶è¯„ä¼°
# å¯¹æµ‹è¯•é›†è¯„ä¼°
mask_all_is_test = np.isin(roads.index.values, idxte)
rk_pred_test = yhat_te + rk_resid_all[mask_all_is_test]

print(f"[RK]  R2(test)  = {r2_score(yte, rk_pred_test):.3f}")
print(f"[RK]  RMSE(test)= {math.sqrt(mean_squared_error(yte, rk_pred_test)):.1f}")

# å…¨ç½‘é¢„æµ‹
X_all = roads[num_cols + [roadtype_col]].copy()
for c in num_cols: X_all[c] = pd.to_numeric(X_all[c], errors="coerce").fillna(0.0)
X_all[roadtype_col] = X_all[roadtype_col].astype(str).fillna("unknown")
yhat_all = reg.predict(X_all)

roads_out = roads.copy()
roads_out["aadt_obs"]     = roads_out["aadt_obs"].astype(float)
roads_out["aadt_pred_reg"]= yhat_all.astype(float)
roads_out["rk_resid"]     = rk_resid_all.astype(float)
roads_out["aadt_pred_rk"] = (roads_out["aadt_pred_reg"] + roads_out["rk_resid"]).astype(float)

# 10) å¯¼å‡º
roads_out.to_file(OUT_PATH, driver="GeoJSON")
print(f"âœ… å·²è¾“å‡ºï¼š{OUT_PATH}")
# ================== ç»“å°¾ï¼šå˜é‡é‡è¦æ€§ & R^2 æ±‡æ€» ==================

print("\n================ å˜é‡é‡è¦æ€§ & RÂ² æ±‡æ€» ================")

# 1) å›å½’ RÂ²ï¼ˆå·²åœ¨å‰é¢æ‰“å°è¿‡ï¼Œè¿™é‡Œæ±‡æ€»ä¸€ä¸‹ï¼‰
reg_r2_train = r2_score(ytr, yhat_tr)
reg_r2_test  = r2_score(yte, yhat_te)
rk_r2_test   = r2_score(yte, rk_pred_test)

print(f"[å›å½’]   RÂ²(train) = {reg_r2_train:.4f}")
print(f"[å›å½’]   RÂ²(test ) = {reg_r2_test :.4f}")
print(f"[å›å½’å…‹é‡Œé‡‘] RÂ²(test ) = {rk_r2_test  :.4f}")

# 2) æå–éšæœºæ£®æ—çš„å˜é‡é‡è¦æ€§ï¼ˆæ˜ å°„å›å¯è¯»çš„ç‰¹å¾åï¼‰
rf  = reg.named_steps["rf"]
prep = reg.named_steps["prep"]

# å°è¯•è‡ªåŠ¨æ‹¿åˆ° ColumnTransformer å±•å¼€çš„ç‰¹å¾å
try:
    feat_names = prep.get_feature_names_out()
    # æ¸…ç†ä¸€ä¸‹å‰ç¼€ 'num__' / 'cat__'
    feat_names = [fn.replace("num__", "").replace("cat__", "") for fn in feat_names]
except Exception:
    # å…¼å®¹è€ç‰ˆæœ¬ sklearnï¼šæ‰‹åŠ¨æ‹¼
    feat_names = []
    # æ•°å€¼åˆ—åŸæ ·
    feat_names.extend(num_cols)
    # ç±»åˆ«åˆ—ï¼ˆone-hotï¼‰å±•å¼€
    try:
        ohe = None
        for name, trans, cols in prep.transformers_:
            if name == "cat":
                ohe = trans
                break
        if hasattr(ohe, "get_feature_names_out"):
            cat_names = list(ohe.get_feature_names_out([roadtype_col]))
        else:
            # è€ç‰ˆæœ¬ï¼šé€€åŒ–æˆç”¨ç±»åˆ«åˆ—åå ä½
            cat_names = [f"{roadtype_col}_<onehot_{i}>" for i in range(rf.n_features_in_ - len(num_cols))]
        feat_names.extend(cat_names)
    except Exception:
        pass

importances = rf.feature_importances_
feat_import_df = pd.DataFrame({"feature": feat_names, "importance": importances})
feat_import_df = feat_import_df.sort_values("importance", ascending=False).reset_index(drop=True)

# æ‰“å°å‰ 30 ä¸ª
print("\n[Top 30 Feature Importance]")
print(feat_import_df.head(30).to_string(index=False))

# 3) å¯é€‰ï¼šæŠŠ one-hot çš„ roadtype æ±‡æ€»æˆä¸€ä¸ªå¤§ç±»çš„é‡è¦æ€§ï¼ŒPOI å®¶æ—åšç²—æ±‡æ€»
def family_of(f):
    if f.startswith("dens100_") or f.startswith("dens300_") or f.startswith("dens800_"):
        return "poi_density"
    if f.startswith("front20_"):
        return "poi_frontage"
    if f.startswith("dist_"):
        return "poi_distance"
    if f.startswith("entropy_"):
        return "poi_entropy"
    if f.startswith(f"{roadtype_col}_") or f.startswith(roadtype_col):
        return "roadtype(onehot)"
    if f == "population":
        return "population"
    if f == "maxspeed_num":
        return "maxspeed"
    if f == roaddens_col:
        return "road_density"
    return "other"

feat_import_df["family"] = feat_import_df["feature"].map(family_of)
family_import = (feat_import_df
                 .groupby("family", as_index=False)["importance"].sum()
                 .sort_values("importance", ascending=False))

print("\n[Feature Importance by Family]")
print(family_import.to_string(index=False))

# 4) ä¿å­˜ç»“æœ
feat_import_df.to_csv("feature_importance_detailed.csv", index=False, encoding="utf-8-sig")
family_import.to_csv("feature_importance_family.csv", index=False, encoding="utf-8-sig")

print("\nğŸ“„ å·²ä¿å­˜ï¼šfeature_importance_detailed.csv, feature_importance_family.csv")
print("=========================================================\n")
